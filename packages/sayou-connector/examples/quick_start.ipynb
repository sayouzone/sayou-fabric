{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "6cea0be7",
   "metadata": {},
   "source": [
    "# Sayou Connector Quick Start\n",
    "\n",
    "**Sayou Fabric**의 데이터 수집기인 `sayou-connector`의 기본 사용법을 다룹니다.\n",
    "이 노트북에서는 다음 세 가지 시나리오를 시연합니다.\n",
    "\n",
    "1. **Local File Scan:** 로컬 폴더의 문서 파일 수집\n",
    "2. **SQLite Scan:** 데이터베이스의 대량 데이터 페이지네이션 수집\n",
    "3. **Web Crawling:** 웹 뉴스 기사 수집 및 링크 탐색"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ad54a550",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import sqlite3\n",
    "import logging\n",
    "import shutil\n",
    "\n",
    "# 사유존 커넥터 임포트\n",
    "from sayou.connector.pipeline import ConnectorPipeline\n",
    "\n",
    "# 로그 레벨 설정 (INFO로 설정하여 주요 흐름만 확인)\n",
    "logging.basicConfig(level=logging.INFO, format='%(asctime)s - %(message)s')\n",
    "\n",
    "print(\"Import Successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1edff9ce",
   "metadata": {},
   "source": [
    "## 1. 초기화 (Initialization)\n",
    "\n",
    "파이프라인을 생성하고 초기화합니다. 이 과정에서 기본 제공되는 플러그인(File, SQLite, Web)들이 자동으로 등록됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4dc77d92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 오케스트레이터 초기화\n",
    "pipeline = ConnectorPipeline()\n",
    "pipeline.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52a17fe5",
   "metadata": {},
   "source": [
    "## 2. 시나리오: Local File Scan\n",
    "\n",
    "특정 폴더에 있는 파일들을 재귀적으로 탐색하여 읽어옵니다.\n",
    "테스트를 위해 임시 파일들을 먼저 생성하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91b1c163",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Helper] 테스트용 더미 파일 생성\n",
    "TEST_ROOT = \"test_docs_notebook\"\n",
    "\n",
    "if os.path.exists(TEST_ROOT):\n",
    "    shutil.rmtree(TEST_ROOT)\n",
    "os.makedirs(TEST_ROOT)\n",
    "\n",
    "with open(f\"{TEST_ROOT}/readme.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(\"Welcome to Sayou Fabric!\")\n",
    "with open(f\"{TEST_ROOT}/config.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write('{\"version\": \"0.1.0\"}')\n",
    "    \n",
    "print(f\"Created dummy files in '{TEST_ROOT}'\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45f4a062",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 실행 (전략: file)\n",
    "# extensions=['.txt']로 설정하여 txt 파일만 수집합니다.\n",
    "\n",
    "file_packets = pipeline.run(\n",
    "    source=TEST_ROOT,\n",
    "    strategy=\"file\",\n",
    "    extensions=[\".txt\"],  # json 파일은 무시됨\n",
    "    recursive=True\n",
    ")\n",
    "\n",
    "print(\"=== File Scan Results ===\")\n",
    "for i, packet in enumerate(file_packets):\n",
    "    if packet.success:\n",
    "        # FileFetcher는 바이너리(bytes)를 반환하므로 디코딩\n",
    "        content = packet.data.decode(\"utf-8\")\n",
    "        print(f\"[{i}] File: {packet.task.meta['filename']}\")\n",
    "        print(f\"    Path: {packet.task.uri}\")\n",
    "        print(f\"    Content: {content}\")\n",
    "    else:\n",
    "        print(f\"[{i}] Error: {packet.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cade647f",
   "metadata": {},
   "source": [
    "## 3. 시나리오: SQLite DB Scan\n",
    "\n",
    "데이터베이스에서 대량의 데이터를 가져옵니다.\n",
    "`batch_size`를 설정하면, Generator가 자동으로 `LIMIT/OFFSET` 쿼리를 생성하여 끊어서 가져옵니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f959ceb",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Helper] 테스트용 DB 생성\n",
    "DB_PATH = \"test_users_notebook.db\"\n",
    "\n",
    "if os.path.exists(DB_PATH):\n",
    "    os.remove(DB_PATH)\n",
    "\n",
    "conn = sqlite3.connect(DB_PATH)\n",
    "cur = conn.cursor()\n",
    "cur.execute(\"CREATE TABLE users (id INTEGER, name TEXT)\")\n",
    "\n",
    "# 25개의 데이터 생성\n",
    "for i in range(25):\n",
    "    cur.execute(\"INSERT INTO users VALUES (?, ?)\", (i, f\"User_{i:03d}\"))\n",
    "conn.commit()\n",
    "conn.close()\n",
    "\n",
    "print(f\"Created dummy DB at '{DB_PATH}' with 25 rows.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab45164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 파이프라인 실행 (전략: sqlite)\n",
    "# batch_size=10으로 설정 (총 25개이므로 3번의 Fetch 발생 예상: 10개, 10개, 5개)\n",
    "\n",
    "db_packets = pipeline.run(\n",
    "    source=DB_PATH,\n",
    "    strategy=\"sqlite\",\n",
    "    query=\"SELECT * FROM users\",\n",
    "    batch_size=10\n",
    ")\n",
    "\n",
    "print(\"=== DB Scan Results ===\")\n",
    "for i, packet in enumerate(db_packets):\n",
    "    if packet.success:\n",
    "        rows = packet.data\n",
    "        offset = packet.task.meta.get('offset')\n",
    "        print(f\"Batch {i+1} (Offset {offset}): Fetched {len(rows)} rows\")\n",
    "        \n",
    "        # 첫 번째 배치의 데이터 샘플 확인\n",
    "        if i == 0 and rows:\n",
    "            print(f\"    Sample Row: {rows[0]}\")\n",
    "    else:\n",
    "        print(f\"Batch {i+1} Failed: {packet.error}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "340042c7",
   "metadata": {},
   "source": [
    "## 4. 시나리오: Web Crawling\n",
    "\n",
    "웹 페이지를 수집하고, 페이지 내의 링크를 찾아 다음 수집 대상을 자동으로 확장합니다.\n",
    "(네트워크 환경에 따라 실패할 수 있습니다.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "08319898",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 다음(Daum) 뉴스 테크 섹션 예시\n",
    "TARGET_URL = \"https://news.daum.net/tech\"\n",
    "\n",
    "# 링크 패턴: 기사 본문 URL만 수집 (정규식)\n",
    "LINK_PATTERN = r\"https://v\\.daum\\.net/v/\\d+\"\n",
    "\n",
    "# CSS 선택자: 제목과 본문 추출\n",
    "SELECTORS = {\n",
    "    \"title\": \".head_view\",     # (사이트 구조 변경 시 수정 필요)\n",
    "    \"content\": \".article_view\"\n",
    "}\n",
    "\n",
    "print(f\"Starting crawl on: {TARGET_URL}\")\n",
    "\n",
    "try:\n",
    "    web_packets = pipeline.run(\n",
    "        source=TARGET_URL,\n",
    "        strategy=\"requests\",\n",
    "        link_pattern=LINK_PATTERN,\n",
    "        selectors=SELECTORS,\n",
    "        max_depth=1  # 1단계 깊이까지만 탐색\n",
    "    )\n",
    "\n",
    "    count = 0\n",
    "    for packet in web_packets:\n",
    "        if not packet.success:\n",
    "            continue\n",
    "        \n",
    "        # 데이터 확인 (메타데이터 제외)\n",
    "        data = packet.data\n",
    "        clean_data = {k: v[:50]+\"...\" for k, v in data.items() if not k.startswith(\"__\")}\n",
    "        \n",
    "        # 제목이 추출된 경우만 출력\n",
    "        if clean_data.get(\"title\"):\n",
    "            print(f\"\\n[{count}] Found Article: {packet.task.uri}\")\n",
    "            print(f\"    Title: {clean_data.get('title')}\")\n",
    "            count += 1\n",
    "            \n",
    "            if count >= 3:\n",
    "                print(\"\\n... (데모를 위해 3개만 출력하고 중단합니다)\")\n",
    "                break\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"Web Crawling Skipped: {e}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5866e848",
   "metadata": {},
   "outputs": [],
   "source": [
    "# [Cleanup] 테스트 파일 정리\n",
    "if os.path.exists(TEST_ROOT):\n",
    "    shutil.rmtree(TEST_ROOT)\n",
    "if os.path.exists(DB_PATH):\n",
    "    os.remove(DB_PATH)\n",
    "\n",
    "print(\"Cleanup complete.\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
