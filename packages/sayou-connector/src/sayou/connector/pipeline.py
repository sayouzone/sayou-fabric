from typing import Any, List, Deque, Set, Optional, Dict
from collections import deque
from sayou.core.base_component import BaseComponent
from .interfaces.base_seeder import BaseSeeder
from .interfaces.base_fetcher import BaseFetcher
from .interfaces.base_generator import BaseGenerator

class ConnectorPipeline(BaseComponent):
    """
    (Orchestrator) Seeder, Fetcher, Generatorë¥¼ ì¡°ë¦½í•©ë‹ˆë‹¤.
    'run' ë©”ì„œë“œëŠ” RAG ëª¨ë“œ(ë‹¨ì¼ ì‹¤í–‰)ì™€ í¬ë¡¤ë§ ëª¨ë“œ(ë°°ì¹˜ ì‹¤í–‰)ë¥¼ ëª¨ë‘ ì§€ì›í•©ë‹ˆë‹¤.
    """
    component_name = "ConnectorPipeline"

    def __init__(self, 
        fetcher: BaseFetcher,
        seeder: Optional[BaseSeeder] = None,
        generator: Optional[BaseGenerator] = None
    ):
        self.seeder = seeder
        self.fetcher = fetcher
        self.generator = generator
        self._log("Pipeline initialized with components.")

    def initialize(self, **kwargs):
        """[ì •ìƒ] ì»´í¬ë„ŒíŠ¸ë“¤ì„ None-safeí•˜ê²Œ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        if self.seeder:
            self.seeder.initialize(**kwargs)
        
        self.fetcher.initialize(**kwargs) # ğŸ‘ˆ RAGì— í•„ìˆ˜
        
        if self.generator:
            self.generator.initialize(**kwargs)

    def run(self, **kwargs) -> Dict[str, Any]:
        """
        [ìˆ˜ì •] íŒŒì´í”„ë¼ì¸ì˜ ë‹¨ì¼ ì§„ì…ì (Router)ì…ë‹ˆë‹¤.
        kwargsì— 'data_source'ê°€ ìˆìœ¼ë©´ RAG ëª¨ë“œë¡œ,
        ì—†ìœ¼ë©´ í¬ë¡¤ë§ ëª¨ë“œë¡œ ì‹¤í–‰ë©ë‹ˆë‹¤.
        """
        data_source = kwargs.get("data_source")
        
        if data_source is not None:
            # 1. RAG ëª¨ë“œ (ë‹¨ì¼ ì‹¤í–‰)
            return self._run_single_fetch(data_source)
        else:
            # 2. í¬ë¡¤ë§ ëª¨ë“œ (ë°°ì¹˜ ì‹¤í–‰)
            max_items = kwargs.get("max_items", 100)
            return self._run_crawl(max_items)

    def _run_single_fetch(self, data_source: Any) -> Dict[str, Any]:
        """
        [ì‹ ê·œ] RAG ì˜ˆì œë¥¼ ìœ„í•œ ë‹¨ì¼ í˜ì¹˜ ë¡œì§ì…ë‹ˆë‹¤.
        """
        self._log(f"Running in single-fetch mode for {data_source}")
        
        raw_data = None
        # BasicRAGê°€ (target, query) íŠœí”Œì„ ì „ë‹¬í•˜ëŠ” ê²½ìš°
        if isinstance(data_source, tuple) and len(data_source) == 2:
            # BaseFetcher.fetch() (ë¼ˆëŒ€)ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
            raw_data = self.fetcher.fetch(target=data_source[0], query=data_source[1])
        else:
            raw_data = self.fetcher.fetch(data_source) # query ì—†ì´ í˜¸ì¶œ

        if raw_data is None:
            raise RuntimeError("Connector failed: empty response")

        # ApiFetcherê°€ ë°˜í™˜í•œ 'raw_data'ì˜ ì›ë³¸ì„ í™•ì¸í•©ë‹ˆë‹¤.
        # print("\n" + "="*20 + " [DEBUG] RAW_DATA FROM CONNECTOR " + "="*20)
        # print(f"Data Type: {type(raw_data)}")
        # print(f"Data Length: {len(raw_data)}")
        # print("\n--- RAW_DATA (START) ---\n")
        # print(raw_data[:500]) # ğŸ‘ˆ ì• 500ì ì¶œë ¥
        # print("\n--- RAW_DATA (END) ---\n")
        # print(raw_data[-500:]) # ğŸ‘ˆ ë’¤ 500ì ì¶œë ¥
        # print("="*66 + "\n")

        return {"raw_data": raw_data}

    def _run_crawl(self, max_items: int) -> Dict[str, Any]:
        """
        [ìˆ˜ì •] ê¸°ì¡´ì˜ í¬ë¡¤ë§(yield) ë¡œì§ì…ë‹ˆë‹¤.
        """
        if not self.seeder:
            raise ValueError("Seeder must be provided for crawl mode.")
            
        self._log(f"Running in crawl mode (max: {max_items} items)...")
        
        crawled_results: List[tuple] = []
        queue: Deque[str] = deque()
        seen: Set[str] = set()
        count = 0

        # ( ... ê¸°ì¡´ í¬ë¡¤ë§ ë¡œì§ ... )
        initial_seeds = self.seeder.seed()
        for seed in initial_seeds:
            if seed not in seen:
                queue.append(seed)
                seen.add(seed)
        
        while queue and count < max_items:
            resource_id = queue.popleft()
            
            # BaseFetcher.fetch() (ë¼ˆëŒ€)ë¥¼ í˜¸ì¶œí•©ë‹ˆë‹¤.
            raw_data = self.fetcher.fetch(resource_id) # query ì—†ì´ í˜¸ì¶œ
            
            if raw_data is None:
                continue 
            
            count += 1
            crawled_results.append((resource_id, raw_data))
            
            if self.generator:
                new_seeds = self.generator.generate(raw_data)
                # ( ... new_seeds ë¡œì§ ... )

        return {"crawled_data": crawled_results}