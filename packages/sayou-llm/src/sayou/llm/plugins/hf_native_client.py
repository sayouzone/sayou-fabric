import torch
from transformers import AutoTokenizer, AutoModelForCausalLM
from typing import Dict, Any, Iterator, List, Optional

# 1. T1 ì¸í„°íŽ˜ì´ìŠ¤ ìž„í¬íŠ¸
from sayou.llm.interfaces.base_llm_client import BaseLLMClient, LLMResponse, StreamChunk
from sayou.llm.core.exceptions import LLMError
# (sayou.core.base_componentëŠ” BaseLLMClientê°€ ì´ë¯¸ ìƒì†ë°›ì•˜ë‹¤ê³  ê°€ì •)

class HuggingFaceNativeClient(BaseLLMClient):
    """
    (Tier 3 - ì»¤ìŠ¤í…€ í”ŒëŸ¬ê·¸ì¸)
    ë¡œì»¬ì— ì €ìž¥ëœ HuggingFace ì›ë³¸(safetensors) ëª¨ë¸ì„
    'AutoModelForCausalLM.generate()'ë¡œ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤.
    
    ì´ ë°©ì‹ì€ ëª¨ë¸ì˜ 'ì±„íŒ… í…œí”Œë¦¿'ì„ T1ì—ì„œ ì™„ë²½í•˜ê²Œ ì œì–´í•  ìˆ˜ ìžˆê²Œ í•©ë‹ˆë‹¤.
    """
    component_name = "HuggingFaceNativeClient"
    SUPPORTED_TYPES = ["hf_native"] # ðŸ‘ˆ ì´ ì»¤ìŠ¤í…€ í”ŒëŸ¬ê·¸ì¸ì˜ íƒ€ìž…

    def initialize(self, **kwargs):
        """
        'model_path' (ë¡œì»¬ ê²½ë¡œ)ë¥¼ ë°›ì•„ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        
        Args:
            model_path (str): (e.g., "C:/models/gemma-3-1b-it")
            device (str, optional): (e.g., "cuda", "cpu")
            torch_dtype (Any, optional): (e.g., torch.bfloat16)
        """
        self.model_path = kwargs.get("model_path")
        if not self.model_path:
            raise LLMError("HuggingFaceNativeClient requires 'model_path'.")
            
        self.device = kwargs.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        self.torch_dtype = kwargs.get("torch_dtype", torch.bfloat16 if self.device == "cuda" else torch.float32)
        
        try:
            self._log(f"Loading Tokenizer from: {self.model_path}")
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_path)
            
            self._log(f"Loading Model to {self.device} from: {self.model_path}...")
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_path,
                device_map=self.device,
                torch_dtype=self.torch_dtype
            )
            self._log("Model loading complete.")
            
        except Exception as e:
            raise LLMError(f"Failed to load Transformers model {self.model_path}: {e}")

    # --- [ T1 ì¸í„°íŽ˜ì´ìŠ¤ êµ¬í˜„ ] ---

    def _prepare_messages(self, prompt: str, chat_history: Optional[List[Dict[str, str]]]) -> Any:
        """[T1 êµ¬í˜„] ëª¨ë¸ ê³ ìœ ì˜ 'ì±„íŒ… í…œí”Œë¦¿'ì„ ì ìš©í•˜ê³  í† í°í™”í•©ë‹ˆë‹¤."""
        
        messages = chat_history or []
        messages.append({"role": "user", "content": prompt})
        
        # â­ï¸ í† í¬ë‚˜ì´ì €ê°€ ì•Œê³  ìžˆëŠ” ê³ ìœ  í…œí”Œë¦¿(Gemma, Llama ë“±)ì„ ì ìš©
        try:
            input_ids = self.tokenizer.apply_chat_template(
                messages,
                add_generation_prompt=True, # ðŸ‘ˆ 'model\n' í”„ë¡¬í”„íŠ¸ ì¶”ê°€
                tokenize=True,
                return_tensors="pt" # ðŸ‘ˆ PyTorch í…ì„œë¡œ ë°˜í™˜
            ).to(self.device)
            return input_ids
        except Exception as e:
            self._log(f"Warning: apply_chat_template failed. Using simple prompt. {e}")
            # (ì±„íŒ… í…œí”Œë¦¿ì´ ì—†ëŠ” ëª¨ë¸ì„ ìœ„í•œ í´ë°±)
            return self.tokenizer(prompt, return_tensors="pt").input_ids.to(self.device)

    def _do_invoke(self, input_ids: torch.Tensor, **kwargs) -> Any:
        """[T1 êµ¬í˜„] 'model.generate()'ë¥¼ ì§ì ‘ í˜¸ì¶œí•©ë‹ˆë‹¤."""
        
        # â­ï¸ (T3 ë¡œì§) ìž…ë ¥ í† í° ê¸¸ì´ë¥¼ ì €ìž¥ (ì‘ë‹µ íŒŒì‹±ì— ì‚¬ìš©)
        self._last_input_length = input_ids.shape[1]
        
        # (ëª¨ë¸ ìƒì„± ì˜µì…˜)
        generation_kwargs = {
            "max_new_tokens": kwargs.get("max_tokens", 512),
            "temperature": kwargs.get("temperature", 0.7),
            "eos_token_id": self.tokenizer.eos_token_id,
        }
        
        # â­ï¸ ì›ë³¸ ëª¨ë¸ ìƒì„±
        outputs = self.model.generate(input_ids, **generation_kwargs)
        return outputs

    def _parse_invoke_response(self, raw_response_tensor: torch.Tensor) -> LLMResponse:
        """[T1 êµ¬í˜„] ì›ë³¸ í…ì„œ ì‘ë‹µì„ 'í‘œì¤€ í¬ë§·'ìœ¼ë¡œ ë³€í™˜"""
        
        # â­ï¸ í•µì‹¬: ìž…ë ¥ í…ì„œ ê¸¸ì´ë¥¼ ì œì™¸í•œ 'ìƒˆë¡œ ìƒì„±ëœ' í† í°ë§Œ ë””ì½”ë”©
        output_ids = raw_response_tensor[0][self._last_input_length:]
        
        text = self.tokenizer.decode(output_ids, skip_special_tokens=True)
        
        return {
            "text": text.strip(),
            "metadata": {
                "model_name": self.model_path,
                "token_usage": None, # (generate()ëŠ” í† í° ìˆ˜ ë°˜í™˜ ì•ˆ í•¨)
                "finish_reason": "stop"
            }
        }

    # (ìŠ¤íŠ¸ë¦¬ë°ì€ TextIteratorStreamerë¥¼ ì‚¬ìš©í•˜ì—¬ ë³„ë„ êµ¬í˜„ í•„ìš”)
    def _do_stream(self, messages: Any, **kwargs) -> Iterator[Any]:
        self._log("Streaming not implemented for HFNativeClient yet.")
        raise NotImplementedError("HFNativeClient streaming not implemented yet.")

    def _parse_stream_response(self, raw_stream: Iterator[Any]) -> Iterator[StreamChunk]:
        raise NotImplementedError