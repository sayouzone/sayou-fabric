# (í•„ìš”) pip install torch transformers accelerate
import torch
from transformers import AutoModelForCausalLM, AutoTokenizer, Pipeline, pipeline
from sayou.llm.interfaces.base_llm_client import BaseLLMClient, LLMResponse, StreamChunk
from sayou.llm.core.exceptions import LLMError
from typing import Dict, Any, Iterator, List, Optional

class TransformersClient(BaseLLMClient):
    """
    (Tier 2 - ê³µì‹ ì–´ëŒ‘í„°)
    'HuggingFace Transformers' (safetensors) ëª¨ë¸ì„
    ë©”ëª¨ë¦¬ì— ì§ì ‘ ë¡œë“œí•˜ì—¬ 'BaseLLMClient'(T1)ë¡œ ëž˜í•‘í•©ë‹ˆë‹¤.
    """
    component_name = "TransformersClient"
    SUPPORTED_TYPES = ["transformers_native"] # ðŸ‘ˆ ì´ í´ë¼ì´ì–¸íŠ¸ì˜ ê³ ìœ  íƒ€ìž…

    def initialize(self, **kwargs):
        """
        'model_name' (HuggingFace ID)ì„ ë°›ì•„ ëª¨ë¸ê³¼ í† í¬ë‚˜ì´ì €ë¥¼ ë¡œë“œí•©ë‹ˆë‹¤.
        """
        self.model_name = kwargs.get("model_name") # (e.g., "gemma-1.1-2b-it")
        if not self.model_name:
            raise LLMError("TransformersClient requires 'model_name'.")
            
        device = kwargs.get("device", "cuda" if torch.cuda.is_available() else "cpu")
        
        try:
            self.tokenizer = AutoTokenizer.from_pretrained(self.model_name)
            self.model = AutoModelForCausalLM.from_pretrained(
                self.model_name,
                device_map=device,
                torch_dtype=torch.bfloat16 # (ì„±ëŠ¥ì„ ìœ„í•œ ì„¤ì • ì˜ˆì‹œ)
            )
            
            # (HuggingFace Pipelineì„ ì‚¬ìš©í•˜ë©´ ë” íŽ¸ë¦¬í•˜ê²Œ ëž˜í•‘ ê°€ëŠ¥)
            self.pipeline = pipeline(
                "text-generation",
                model=self.model,
                tokenizer=self.tokenizer,
                device_map=device
            )
        except Exception as e:
            raise LLMError(f"Failed to load Transformers model {self.model_name}: {e}")
            
        self._log(f"TransformersClient (T2 Adapter) initialized. Model: {self.model_name} on {device}")

    def _prepare_messages(self, prompt: str, chat_history: Optional[List[Dict[str, str]]]) -> Any:
        """[T1 êµ¬í˜„] Transformers Pipelineì´ ì‚¬ìš©í•  'messages' í¬ë§· ìƒì„±"""
        # (ì±„íŒ… ížˆìŠ¤í† ë¦¬ì™€ ì¿¼ë¦¬ë¥¼ í…œí”Œë¦¿ì— ë§žê²Œ ì¡°í•©)
        messages = []
        # for msg in chat_history:
        #     messages.append({"role": msg["role"], "content": msg["content"]})
        messages.append({"role": "user", "content": prompt})
        
        # â­ï¸ Pipeline.tokenizer.apply_chat_templateì„ ì‚¬ìš©
        return self.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)

    def _do_invoke(self, messages_prompt: str, **kwargs) -> Any:
        """[T1 êµ¬í˜„] Transformers Pipeline (model.generate) ì‹¤ì œ í˜¸ì¶œ"""
        
        # â­ï¸ HuggingFace Pipeline í˜¸ì¶œ
        return self.pipeline(
            messages_prompt,
            max_new_tokens=kwargs.get("max_tokens", 512),
            do_sample=True,
            temperature=kwargs.get("temperature", 0.7),
            # (Pipelineì€ 'usage' ì •ë³´ë¥¼ ê¸°ë³¸ìœ¼ë¡œ ë°˜í™˜í•˜ì§€ ì•ŠìŒ)
        )

    def _parse_invoke_response(self, raw_response: List[Dict[str, Any]]) -> LLMResponse:
        """[T1 êµ¬í˜„] Pipeline ì‘ë‹µì„ 'í‘œì¤€ í¬ë§·'ìœ¼ë¡œ ë³€í™˜"""
        # (raw_response[0]['generated_text'])
        # (íŒŒì´í”„ë¼ì¸ì´ í”„ë¡¬í”„íŠ¸ê¹Œì§€ ë°˜í™˜í•˜ë¯€ë¡œ, ë‹µë³€ ë¶€ë¶„ë§Œ ì¶”ì¶œí•˜ëŠ” ë¡œì§ í•„ìš”)
        full_text = raw_response[0]['generated_text']
        
        # (ì´ ë¶€ë¶„ì€ ëª¨ë¸ í…œí”Œë¦¿ì— ë”°ë¼ ë³µìž¡í•´ì§ˆ ìˆ˜ ìžˆìŒ - í”„ë¡¬í”„íŠ¸ ì œê±°)
        answer_text = full_text # (ìž„ì‹œë¡œ ì „ì²´ í…ìŠ¤íŠ¸ ë°˜í™˜)
        
        return {
            "text": answer_text.strip(),
            "metadata": {
                "model_name": self.model_name,
                "token_usage": None, # â­ï¸ TransformersëŠ” í† í° ìˆ˜ ì¶”ì ì´ ì–´ë ¤ì›€
                "finish_reason": "stop"
            }
        }
    
    # (ìŠ¤íŠ¸ë¦¬ë° êµ¬í˜„ì€ TextIteratorStreamer ë“±ì„ ì‚¬ìš©í•˜ì—¬ ë³„ë„ êµ¬í˜„ í•„ìš”)
    @abstractmethod
    def _do_stream(self, messages: Any, **kwargs) -> Iterator[Any]:
        raise NotImplementedError("TransformersClient streaming not implemented yet.")

    @abstractmethod
    def _parse_stream_response(self, raw_stream: Iterator[Any]) -> Iterator[StreamChunk]:
        raise NotImplementedError