import os
from openai import OpenAI
from sayou.llm.interfaces.base_llm_client import BaseLLMClient, LLMResponse, StreamChunk
from typing import Dict, Any, Iterator, List, Optional
from sayou.llm.core.exceptions import LLMError

class OpenAIClient(BaseLLMClient):
    """
    (Tier 2 - ê¸°ë³¸ ì–´ëŒ‘í„°) 'OpenAI' APIë¥¼
    'BaseLLMClient'(T1) ì¸í„°í˜ì´ìŠ¤ í‘œì¤€ì— ë§ê²Œ êµ¬í˜„í•©ë‹ˆë‹¤.
    """
    component_name = "OpenAIClient"
    SUPPORTED_TYPES = ["openai_chat"] # ğŸ‘ˆ ì´ í´ë¼ì´ì–¸íŠ¸ê°€ ì²˜ë¦¬í•  íƒ€ì…

    def initialize(self, **kwargs):
        """OpenAI í´ë¼ì´ì–¸íŠ¸ ì´ˆê¸°í™”"""
        api_key = kwargs.get("openai_api_key", os.environ.get("OPENAI_API_KEY"))
        if not api_key:
            raise LLMError("OpenAIClient requires 'openai_api_key'.")
            
        self.client = OpenAI(api_key=api_key)
        self.model_name = kwargs.get("model_name", "gpt-4-turbo")
        self._log(f"OpenAIClient (Default Adapter) initialized for model: {self.model_name}")

    def _prepare_messages(self, prompt: str, chat_history: Optional[List[Dict[str, str]]]) -> List[Dict[str, str]]:
        """[T1 êµ¬í˜„] OpenAIì˜ 'messages' í¬ë§· ìƒì„±"""
        messages = []
        # (ì‹¤ì œë¡œëŠ” chat_history í¬ë§· ë³€í™˜ ë¡œì§ í•„ìš”)
        messages.append({"role": "user", "content": prompt})
        return messages

    def _do_invoke(self, messages: List[Dict[str, str]], **kwargs) -> Any:
        """[T1 êµ¬í˜„] OpenAI API ì‹¤ì œ í˜¸ì¶œ"""
        return self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            stream=False,
            **kwargs # (e.g., temperature, max_tokens)
        )

    def _parse_invoke_response(self, raw_response: Any) -> LLMResponse:
        """[T1 êµ¬í˜„] OpenAI ì‘ë‹µì„ 'í‘œì¤€ í¬ë§·'ìœ¼ë¡œ ë³€í™˜"""
        text = raw_response.choices[0].message.content
        usage = raw_response.usage
        
        return {
            "text": text,
            "metadata": {
                "model_name": raw_response.model,
                "token_usage": {
                    "prompt_tokens": usage.prompt_tokens,
                    "completion_tokens": usage.completion_tokens,
                    "total_tokens": usage.total_tokens
                },
                "finish_reason": raw_response.choices[0].finish_reason
            }
        }

    def _do_stream(self, messages: List[Dict[str, str]], **kwargs) -> Iterator[Any]:
        """[T1 êµ¬í˜„] OpenAI ìŠ¤íŠ¸ë¦¼ API í˜¸ì¶œ"""
        return self.client.chat.completions.create(
            model=self.model_name,
            messages=messages,
            stream=True,
            **kwargs
        )

    def _parse_stream_response(self, raw_stream: Iterator[Any]) -> Iterator[StreamChunk]:
        """[T1 êµ¬í˜„] OpenAI ìŠ¤íŠ¸ë¦¼ì„ 'í‘œì¤€ ì²­í¬'ë¡œ ë³€í™˜ (yield)"""
        for chunk in raw_stream:
            delta = chunk.choices[0].delta.content
            
            if delta: # (ë‚´ìš©ì´ ìˆëŠ” ì²­í¬ë§Œ)
                yield {
                    "delta": delta,
                    "metadata": {
                        "finish_reason": chunk.choices[0].finish_reason
                    }
                }