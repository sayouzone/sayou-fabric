{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "3a02d69a",
   "metadata": {},
   "source": [
    "# Sayou Wrapper Quick Start\n",
    "\n",
    "**Sayou Fabric**ì˜ ì§€ì‹ í¬ìž¥ ë‹¨ê³„ì¸ `sayou-wrapper`ë¥¼ ì‹œì—°í•©ë‹ˆë‹¤.\n",
    "ì´ì „ ë‹¨ê³„(`sayou-chunking`)ì—ì„œ ìž˜ë¦° ì²­í¬ë“¤ì„ ìž…ë ¥ë°›ì•„, ì‚¬ìœ ì¡´ í‘œì¤€ ìŠ¤í‚¤ë§ˆì¸ **SayouNode**ë¡œ ë³€í™˜í•©ë‹ˆë‹¤. ì´ ê³¼ì •ì—ì„œ ë‹¨ìˆœ í…ìŠ¤íŠ¸ëŠ” **Ontology Class**(`Topic`, `Table` ë“±)ë¥¼ ë¶€ì—¬ë°›ê³ , ë¬¸ë§¥ì€ **Predicate**(`hasParent`)ë¡œ ì—°ê²°ë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02d5143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import logging\n",
    "from sayou.wrapper.pipeline import WrapperPipeline\n",
    "\n",
    "# ë¡œê·¸ ì„¤ì •\n",
    "logging.basicConfig(level=logging.INFO, format='%(message)s')\n",
    "\n",
    "print(\"Import Successful!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "701069e1",
   "metadata": {},
   "source": [
    "## 1. íŒŒì´í”„ë¼ì¸ ì´ˆê¸°í™”\n",
    "\n",
    "Wrapper íŒŒì´í”„ë¼ì¸ì„ ìƒì„±í•©ë‹ˆë‹¤. ê¸°ë³¸ì ìœ¼ë¡œ `DocumentChunkAdapter`ê°€ ë¡œë“œë©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1562b29e",
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = WrapperPipeline()\n",
    "pipeline.initialize()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "921b467d",
   "metadata": {},
   "source": [
    "## 2. ë°ì´í„° ì¤€ë¹„ (Simulation)\n",
    "\n",
    "`sayou-chunking`ì˜ ê²°ê³¼ë¬¼(JSON íŒŒì¼)ì´ ìžˆë‹¤ê³  ê°€ì •í•˜ê³  ë¡œë“œí•˜ê±°ë‚˜, í…ŒìŠ¤íŠ¸ìš© ë”ë¯¸ ë°ì´í„°ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "570e0d66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# ì‹œë‚˜ë¦¬ì˜¤: Chunking ê²°ê³¼ë¬¼ (Header 1ê°œ + Child Text 2ê°œ)\n",
    "chunks_data = [\n",
    "    {\n",
    "        \"content\": \"# Introduction to AI\",\n",
    "        \"metadata\": {\n",
    "            \"chunk_id\": \"doc1_h_0\",\n",
    "            \"semantic_type\": \"h1\",\n",
    "            \"is_header\": True,\n",
    "            \"source\": \"ai_intro.md\",\n",
    "            \"page_num\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"AI stands for Artificial Intelligence.\",\n",
    "        \"metadata\": {\n",
    "            \"chunk_id\": \"doc1_p_0\",\n",
    "            \"semantic_type\": \"text\",\n",
    "            \"parent_id\": \"doc1_h_0\",  # ë¶€ëª¨(Header)ë¥¼ ê°€ë¦¬í‚´\n",
    "            \"source\": \"ai_intro.md\",\n",
    "            \"page_num\": 1\n",
    "        }\n",
    "    },\n",
    "    {\n",
    "        \"content\": \"| Model | Type |\",\n",
    "        \"metadata\": {\n",
    "            \"chunk_id\": \"doc1_t_0\",\n",
    "            \"semantic_type\": \"table\",\n",
    "            \"parent_id\": \"doc1_h_0\",\n",
    "            \"source\": \"ai_intro.md\"\n",
    "        }\n",
    "    }\n",
    "]\n",
    "\n",
    "# (ì˜µì…˜) ì‹¤ì œ íŒŒì¼ì—ì„œ ë¡œë“œí•˜ëŠ” ê²½ìš°\n",
    "# input_file = \"chunks_output.json\"\n",
    "# if os.path.exists(input_file):\n",
    "#     with open(input_file, \"r\", encoding=\"utf-8\") as f:\n",
    "#         chunks_data = json.load(f)\n",
    "\n",
    "print(f\"Ready to process {len(chunks_data)} chunks.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "423fc23e",
   "metadata": {},
   "source": [
    "## 3. ì‹¤í–‰ (Run Wrapper)\n",
    "\n",
    "`document_chunk` ì „ëžµì„ ì‚¬ìš©í•˜ì—¬ ë³€í™˜ì„ ìˆ˜í–‰í•©ë‹ˆë‹¤.\n",
    "* **Text** -> `sayou:TextFragment`\n",
    "* **Header** -> `sayou:Topic`\n",
    "* **Table** -> `sayou:Table`\n",
    "* **Relationship** -> `sayou:hasParent` ì—°ê²°"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "64d47eca",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    # ì‹¤í–‰\n",
    "    output = pipeline.run(chunks_data, strategy=\"document_chunk\")\n",
    "    \n",
    "    print(f\"âœ… Generated {len(output.nodes)} SayouNodes.\\n\")\n",
    "    \n",
    "    # ê²°ê³¼ ê²€ì¦\n",
    "    for node in output.nodes:\n",
    "        print(f\"ðŸ”¹ [{node.node_class}] {node.node_id}\")\n",
    "        print(f\"   Name: {node.friendly_name}\")\n",
    "        \n",
    "        # ì†ì„± ì¶œë ¥\n",
    "        if \"schema:text\" in node.attributes:\n",
    "            print(f\"   Text: {node.attributes['schema:text'][:30]}...\")\n",
    "        \n",
    "        # ê´€ê³„ ì¶œë ¥\n",
    "        if node.relationships:\n",
    "            print(f\"   Rels: {node.relationships}\")\n",
    "        print(\"-\" * 40)\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"âŒ Error: {e}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18d0beed",
   "metadata": {},
   "source": [
    "## 4. ìµœì¢… ì‚°ì¶œë¬¼ ì €ìž¥\n",
    "\n",
    "ì´ ë°ì´í„°ëŠ” ì´ì œ `sayou-assembler`ë‚˜ `sayou-loader`ë¡œ ë„˜ì–´ê°ˆ ì¤€ë¹„ê°€ ë˜ì—ˆìŠµë‹ˆë‹¤."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d602fba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Pydantic Model -> JSON String\n",
    "final_json = output.model_dump_json(indent=2)\n",
    "\n",
    "with open(\"wrapper_output.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(final_json)\n",
    "    \n",
    "print(\"Saved to wrapper_output.json\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
