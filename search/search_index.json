{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Home","text":""},{"location":"#sayou-data-platform","title":"Sayou Data Platform","text":"<p>Sayou Data Platform is a modular framework for building and operating LLM data pipelines. It provides lightweight, composable, and production-ready components for each stage of the data flow \u2014 from ingestion to generation.</p> <p>Get Started View on GitHub</p>"},{"location":"#1-overview","title":"1. Overview","text":"<p>Sayou Data Platform decomposes the LLM pipeline into modular data units, allowing developers to build explicit and traceable workflows. Each module is an independent Python package with minimal dependencies.</p> Layer Description Connector Data ingestion (API, file, or database sources) Wrapper Standardized data representation and validation Chunking Text segmentation and preprocessing Refinery Data cleaning and transformation Assembler Structural data composition (e.g., knowledge graphs) Loader Data persistence (e.g., VectorDB, file) Extractor Query and retrieval interface LLM Model adapter layer for generation RAG Execution engine for multi-step workflows"},{"location":"#2-design-principles","title":"2. Design Principles","text":"<p>Explicit Composition All components are connected explicitly. There is no implicit \u201cmagic\u201d behavior in the pipeline.</p> <p>Three-Tier Architecture Every module follows a consistent interface hierarchy:</p> Tier Purpose Example Tier 1 \u2013 Interface Defines base contracts <code>BaseFetcher</code>, <code>BaseLLMClient</code> Tier 2 \u2013 Default Implementation Official, production-tested implementations <code>RecursiveSplitter</code>, <code>OpenAIAdapter</code> Tier 3 \u2013 Plugin Layer Extend or override system behavior Custom database connectors or internal LLMs <p>Lightweight &amp; Independent Modules can be installed individually:</p> <pre><code>pip install sayou-chunking\npip install sayou-llm\n</code></pre>"},{"location":"#3-ecosystem-packages","title":"3. Ecosystem Packages","text":"Package Status Description <code>sayou-core</code> Common base utilities (BaseComponent, Atom) <code>sayou-connector</code> Data ingestion (API, File, DB...) <code>sayou-wrapper</code> Data validation and wrapping <code>sayou-chunking</code> Text segmentation and chunking <code>sayou-refinery</code> Data cleansing and transformation <code>sayou-assembler</code> Structured data assembly (KG builder...) <code>sayou-loader</code> Data loading to VectorDB / file systems <code>sayou-extractor</code> Query and retrieval pipeline <code>sayou-llm</code> LLM client adapters (OpenAI, HF, Local) <code>sayou-rag</code> RAG pipeline execution and orchestration"},{"location":"#4-documentation-structure","title":"4. Documentation Structure","text":"<ul> <li>Get Started: Installation and system setup</li> <li>Architecture: Design philosophy and interfaces</li> <li>Modules: Guides for each library</li> <li>Reference: API-level documentation</li> </ul>"},{"location":"#maintained-by","title":"\ud83d\udee0 Maintained by","text":"<p>Sayou Technologies Inc. &gt; Open-source initiative by the Sayou AI Infrastructure Team GitHub Repository \u2192</p>"},{"location":"get-started/architecture/","title":"Architecture","text":"<p>You don't need to understand the architecture to use <code>BasicRAG</code>, but it helps to know what's happening under the hood.</p> <p>The Sayou framework is a composable set of libraries, or \"Lego bricks.\" The <code>BasicRAG</code> facade simply assembles these bricks for you in a pre-defined order.</p> <p>This is the pipeline <code>BasicRAG</code> builds for you:</p>"},{"location":"get-started/architecture/#the-basicrag-pipeline","title":"The <code>BasicRAG</code> Pipeline","text":"<p>1. Connector (<code>sayou-connector</code>)</p> <ul> <li> <p>Job: Fetches raw data.</p> </li> <li> <p>In <code>BasicRAG</code>: <code>BasicRAG</code> uses the <code>ApiFetcher</code> component. When you pass <code>data_source=(URL, QUERY)</code> to <code>pipeline.run()</code>, this component executes the API call and returns the raw JSON string.</p> </li> </ul> <p>2. Wrapper (<code>sayou-wrapper</code>)</p> <ul> <li> <p>Job: Parses and structures the raw data.</p> </li> <li> <p>In <code>BasicRAG</code>: This is the most complex stage, which <code>BasicRAG</code> fully automates.</p> <ol> <li>It receives the single JSON string from the Connector.</li> <li>It parses the outer JSON (<code>{\"header\": ..., \"body\": ...}</code>).</li> <li>It extracts the <code>body.paths</code> field.</li> <li>It handles multi-encoded JSON (e.g., if <code>paths</code> is a string <code>\"[...]\"</code>) by parsing it again until it gets a true list.</li> <li>It passes this list (<code>[{...}, {...}]</code>) to the <code>BaseMapper</code>'s <code>map_list</code> function.</li> <li>The <code>BaseMapper</code> uses your <code>map_logic</code> function (via the internal <code>LambdaMapper</code>) to transform each item (<code>{...}</code>) into a validated <code>DataAtom</code>.</li> </ol> </li> </ul> <p>3. Refinery (<code>sayou-refinery</code>)</p> <ul> <li> <p>Job: Cleans the <code>DataAtoms</code>.</p> </li> <li> <p>In <code>BasicRAG</code>: By default, <code>BasicRAG</code> loads a <code>DefaultTextCleaner</code> to remove HTML tags (like <code>&lt;b&gt;</code>) from the <code>friendly_name</code> field.</p> </li> </ul> <p>4. Assembler (<code>sayou-assembler</code>)</p> <ul> <li> <p>Job: Builds the Knowledge Graph (KG) from the <code>DataAtoms</code>.</p> </li> <li> <p>In <code>BasicRAG</code>:</p> <ol> <li>The <code>SchemaValidator</code> (auto-loaded) checks if each Atom has an <code>entity_class</code> (which you provided in <code>map_logic</code>).</li> <li>The <code>DefaultKGBuilder</code> constructs the graph.</li> <li>The <code>FileStorer</code> saves the final <code>final_kg.json</code> to disk.</li> </ol> </li> </ul> <p>5. RAG Stage (<code>sayou-rag</code> + <code>llm</code> + <code>extractor</code>)</p> <ul> <li> <p>Job: Uses the KG to answer the query.</p> </li> <li> <p>In <code>BasicRAG</code>:</p> <ol> <li>The <code>FileRetriever</code> (from <code>sayou-extractor</code>) reads the <code>final_kg.json</code>.</li> <li>The <code>SimpleKGContextFetcher</code> formats the KG data into a clean text <code>context</code>.</li> <li>The <code>LLMPipeline</code> (from <code>sayou-llm</code>) injects this <code>context</code> into a prompt for your local LLM.</li> <li>The final <code>answer</code> is returned.</li> </ol> </li> </ul> <p>When you're ready, you can \"graduate\" from <code>BasicRAG</code> and assemble these components yourself. This is covered in the Library Guides and Sayou Agent sections.</p>"},{"location":"get-started/installation/","title":"Installation","text":"<p>Getting started with Sayou is straightforward. The <code>sayou-rag</code> package is the main \"umbrella package\" that includes all the core components you need to build your first pipeline.</p>"},{"location":"get-started/installation/#install-with-pip","title":"Install with pip","text":"<p>We recommend installing <code>sayou-rag</code> in a virtual environment.</p> <pre><code># Create and activate a virtual environment (recommended)\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\n\n# Install the main package\npip install sayou-rag\n</code></pre> <p>This single command installs <code>sayou-rag</code> and all its core library dependencies, such as: * <code>sayou-core</code> * <code>sayou-connector</code> * <code>sayou-wrapper</code> * <code>sayou-assembler</code> * <code>sayou-extractor</code> * <code>sayou-llm</code></p>"},{"location":"get-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.9 or higher.</li> </ul>"},{"location":"get-started/introduction/","title":"Introduction","text":"<p>Welcome to the Sayouzone's Data Platform.</p> <p>Sayou is a modular framework for building production-grade, scalable RAG (Retrieval-Augmented Generation) pipelines.</p>"},{"location":"get-started/introduction/#what-sayou-is","title":"What Sayou Is","text":"<p>Sayou is a composable set of libraries designed to orchestrate the full data-to-answer lifecycle. This includes:</p> <ol> <li>Connecting to live data sources (APIs, DBs, files).</li> <li>Structuring raw data (parsing, validation, mapping).</li> <li>Refining and cleaning (e.g., text, metadata).</li> <li>Assembling data into a queryable 'Knowledge Asset' (like a KG).</li> <li>Extracting precise context (e.g., file retrieval, KG queries).</li> <li>Orchestrating the final LLM-powered response.</li> </ol>"},{"location":"get-started/introduction/#why-sayou-is-modular","title":"Why Sayou is Modular","text":"<p>This specialized, multi-library architecture (10+ libraries) is a deliberate design choice.</p> <p>We believe that robust, scalable AI systems require a robust, scalable data engineering process. This modularity ensures that every component of the pipeline is testable, replaceable, and extensible.</p> <p>This architecture is built on two layers of a 3-Tier (Interface -&gt; Default -&gt; Plugin) structure:</p> <ol> <li> <p>Micro-level: Each individual library (e.g., <code>sayou-connector</code>) follows the 3-Tier principle, allowing developers to plug in custom components.</p> </li> <li> <p>Macro-level: The entire system forms a 3-Tier structure (Core -&gt; Libraries -&gt; RAG), allowing developers to assemble entirely new pipelines.</p> </li> </ol>"},{"location":"get-started/introduction/#getting-started","title":"Getting Started","text":"<p>This <code>Get Started</code> guide focuses on the <code>BasicRAG</code> facade. This facade composes 5+ of these libraries into a simple, pre-configured pipeline, allowing you to build your first \"API-to-Answer\" workflow in minutes.</p>"},{"location":"get-started/philosophy/","title":"Philosophy","text":"<p>Sayouzone's Data Platform is built on three core principles that define its value and design.</p>"},{"location":"get-started/philosophy/#1-the-facade-principle-low-floor-high-ceiling","title":"1. The Facade Principle: \"Low Floor, High Ceiling\"","text":"<p>Our goal is to provide the simplest possible entry point for a complex task.</p> <ul> <li> <p>Low Floor (Easy Start): The <code>BasicRAG</code> facade. You import one class (<code>BasicRAG</code>) and provide one function (<code>map_logic</code>). The framework handles the rest. This gets you from \"API-to-Answer\" in minutes.</p> </li> <li> <p>High Ceiling (High Customization): When you outgrow the facade, you have full access to the underlying libraries (Lego bricks). Want to swap the <code>FileStorer</code> for a <code>Neo4jStorer</code>? Import <code>AssemblerPipeline</code> and configure it yourself. You are never locked into the \"basic\" flow.</p> </li> </ul>"},{"location":"get-started/philosophy/#2-built-for-production-and-scale","title":"2. Built for Production and Scale","text":"<p>Sayou is engineered for production-grade systems. We believe that a scalable RAG system is a scalable data engineering system.</p> <p>This \"Process-Driven\" philosophy is why Sayou is not a single library. It is a composable set of 11+ specialized modules (Connector, Wrapper, Assembler...). Each module is built with a 3-Tier (Interface -&gt; Default -&gt; Plugin) architecture, ensuring that every single component of your pipeline can be tested, customized, and replaced as your application scales.</p>"},{"location":"get-started/philosophy/#3-the-knowledge-graph-as-a-reusable-asset","title":"3. The Knowledge Graph as a Reusable Asset","text":"<p>A Sayou pipeline doesn't just produce an answer in memory. It produces a tangible, structured, and reusable data asset: the Knowledge Graph (<code>final_kg.json</code>).</p> <p>This KG is not a \"black box\" like a vector database index. It's a clean, auditable JSON file that can be:</p> <ul> <li> <p>Queried again later without re-running the API.</p> </li> <li> <p>Imported into a graph database like Neo4j.</p> </li> <li> <p>Used for analytics or visualization.</p> </li> <li> <p>Versioned and managed as part of your data infrastructure.</p> </li> </ul>"},{"location":"get-started/quickstart/","title":"Quickstart","text":"<p>Welcome to the Sayou <code>BasicRAG</code> Quickstart. This guide will walk you through the core value of Sayou: turning a complex API into an LLM-powered answer in just a few steps.</p> <p>We will build a pipeline that: 1.  Fetches live data from the Seoul Subway API. 2.  Builds a Knowledge Graph (KG) from the JSON response. 3.  Answers a question using the KG and a local LLM.</p> <p>The <code>BasicRAG</code> facade handles all the complexity, so you only need to focus on Step 2.</p>"},{"location":"get-started/quickstart/#step-1-installation","title":"Step 1: Installation","text":"<p>First, install the <code>sayou-rag</code> umbrella package, which includes all necessary components.</p> <pre><code>pip install sayou-rag\n</code></pre>"},{"location":"get-started/quickstart/#step-2-define-your-mapping-logic-your-only-job","title":"Step 2: Define Your Mapping Logic (Your Only Job)","text":"<p><code>BasicRAG</code> handles everything except one task: it doesn't know how to parse your specific API's JSON.</p> <p>You provide this logic as a simple Python function (<code>map_logic</code>). This function receives one item from the API response (<code>row</code>) and returns a dictionary for the Knowledge Graph.</p> <p>You do not need to <code>import BaseMapper</code> or understand the <code>sayou-wrapper</code> library to do this.</p> <pre><code>import os\nimport json\nfrom typing import Any, Dict\n\ndef seoul_subway_logic(row: Any) -&gt; Dict[str, Any]:\n    \"\"\"\n    This is your 'map_logic'.\n    It parses one item from the API's 'paths' list.\n    \"\"\"\n    if not isinstance(row, dict):\n        print(f\"Skipping item, expected dict, got {type(row)}\")\n        return None\n    try:\n        # We define what the KG will look like.\n        return {\n            \"source\": \"seoul_api\",\n            \"type\": \"subway_path\",\n            \"payload\": {\n                \"entity_id\": f\"sayou:path:{row['dptreStn']['stnNm']}_{row['arvlStn']['stnNm']}\",\n                \"entity_class\": \"sayou:Path\", # Required by the Assembler\n                \"friendly_name\": f\"Path: {row['dptreStn']['stnNm']} -&gt; {row['arvlStn']['stnNm']}\",\n                \"attributes\": {\"sayou:totalTime\": row.get(\"totalreqHr\", 120)}\n            }\n        }\n    except KeyError as e:\n        print(f\"Mapping failed, missing key {e} in item: {row}\")\n        return None\n</code></pre>"},{"location":"get-started/quickstart/#step-3-prepare-configuration-the-boilerplate","title":"Step 3: Prepare Configuration (The \"Boilerplate\")","text":"<p>This is the \"boring\" part\u2014telling Sayou where everything is.</p> <p>Create a file (<code>quickstart.py</code>) and add your <code>seoul_subway_logic</code> function from Step 2. Then, add this configuration boilerplate below it.</p> <pre><code># --- Configuration Boilerplate ---\n\n# 1. API and Model Paths\n# (Get API key from [https://data.seoul.go.kr/](https://data.seoul.go.kr/))\nSEOUL_API_KEY = os.environ.get(\"SEOUL_API_KEY\") \nif not SEOUL_API_KEY:\n    raise ValueError(\"SEOUL_API_KEY environment variable is not set.\")\n\nAPI_TARGET_URL = f\"[http://openapi.seoul.go.kr:8088/](http://openapi.seoul.go.kr:8088/){SEOUL_API_KEY}/json/getShtrmPath\"\nAPI_QUERY = {\"url_paths\": [\"1\", \"5\", \"\ub9c8\uace1\", \"\uac15\ub0a8\", \"2025-10-31 10:00:00\"]}\n\n# (Required: Update this to your local model's path)\nLOCAL_MODEL_PATH = \"C:/Your/Local/LLM/Model/Path\" \nOUTPUT_DIR = \"./sayou_quickstart_output\"\n\n# 2. Create Output Directory\nos.makedirs(OUTPUT_DIR, exist_ok=True)\nSCHEMA_PATH = os.path.join(OUTPUT_DIR, \"schema.json\")\nKG_PATH = os.path.join(OUTPUT_DIR, \"final_kg.json\")\n\n# 3. Create Minimal Schema\n# The Assembler needs to know what 'sayou:Path' is.\nschema_content = {\n    \"version\": \"0.0.1\",\n    \"classes\": {\"sayou:Path\": {\"description\": \"A subway path.\"}},\n    \"predicates\": {\"sayou:totalTime\": {}}\n}\nwith open(SCHEMA_PATH, \"w\", encoding=\"utf-8\") as f:\n    json.dump(schema_content, f, indent=2)\n</code></pre>"},{"location":"get-started/quickstart/#step-4-the-3-line-magic-create-init-run","title":"Step 4: The 3-Line Magic (Create, Init, Run)","text":"<p>Now, add the final three steps to your <code>quickstart.py</code> file. This is where the <code>BasicRAG</code> facade performs the entire API-to-Answer pipeline.</p> <pre><code># 1. Import the Facade\nfrom sayou.rag.pipeline.basic import BasicRAG\n\nprint(\"\ud83d\ude80 Starting Sayou RAG Pipeline (API-to-Answer)...\")\n\n# 2. Create: Pass your logic function from Step 2.\n#    (BasicRAG auto-loads default components for validation, cleaning, etc.)\npipeline = BasicRAG(\n    map_logic=seoul_subway_logic\n)\n\n# 3. Initialize: Pass all the configs from Step 3.\npipeline.initialize_all(\n    model_path=LOCAL_MODEL_PATH,\n    base_dir=OUTPUT_DIR,\n    ontology_path=SCHEMA_PATH,\n    filepath=KG_PATH,\n    target_field=\"payload.friendly_name\" # For the default text cleaner\n)\n\n# 4. Run: Provide your query and API info.\nfinal_result = pipeline.run(\n    query=\"Tell me the path from Magok to Gangnam.\",\n    data_source=(API_TARGET_URL, API_QUERY)\n)\n\n# 5. Get the result!\nprint(\"\\n\" + \"=\"*30)\nprint(f\"\u2705 RAG Answer: {final_result['answer']}\")\nprint(f\"\u2705 Knowledge Graph created at: {KG_PATH}\")\n</code></pre>"},{"location":"get-started/quickstart/#next-steps","title":"Next Steps","text":"<p>You have successfully built an API-to-Answer RAG pipeline. - To see how <code>BasicRAG</code> handles advanced customization (like adding your own text cleaner), continue to the Sayou Agent guide. - To learn about the \"Lego bricks\" (<code>Connector</code>, <code>Assembler</code>, etc.) that <code>BasicRAG</code> uses, see the Library Guides.</p>"},{"location":"library-guides/assembler/","title":"Library Guide: sayou-assembler","text":"<p><code>sayou-assembler</code> acts as the architect of the Knowledge Graph. It receives a bag of loose nodes from the Wrapper and creates a structured, interconnected network.</p> <p>It sits right before <code>sayou-loader</code>. While the Loader's job is I/O (writing to DB), the Assembler's job is Logic (defining topology).</p>"},{"location":"library-guides/assembler/#1-core-concepts-architecture","title":"1. Core Concepts &amp; Architecture","text":"<p>The library uses a Dynamic Strategy Pattern to allow different ways of building graphs.</p>"},{"location":"library-guides/assembler/#tier-1-interfaces","title":"Tier 1: Interfaces","text":"<ul> <li><code>BaseBuilder</code>: The interface for all assembly strategies. It accepts <code>WrapperOutput</code> and returns a dictionary representation of the <code>KnowledgeGraph</code>.</li> </ul>"},{"location":"library-guides/assembler/#tier-2-builders-strategies","title":"Tier 2: Builders (Strategies)","text":"<ul> <li><code>HierarchyBuilder</code>: The default strategy.<ul> <li>Iterates through all nodes.</li> <li>Connects nodes based on the <code>relationships</code> dictionary (e.g., <code>sayou:hasParent</code>).</li> <li>Auto-Reverse Linking: Automatically creates reverse edges (e.g., <code>sayou:hasChild</code>) to allow top-down traversal in RAG.</li> </ul> </li> </ul>"},{"location":"library-guides/assembler/#core-model-knowledgegraph","title":"Core Model: <code>KnowledgeGraph</code>","text":"<p>An agnostic, in-memory representation of the graph.</p> <pre><code>class KnowledgeGraph:\n    nodes: Dict[str, GraphNode]\n    edges: List[GraphEdge]\n\n    def to_dict(self): ...\n</code></pre>"},{"location":"library-guides/assembler/#2-usage-examples","title":"2. Usage Examples","text":""},{"location":"library-guides/assembler/#21-building-a-document-hierarchy-graph","title":"2.1. Building a Document Hierarchy Graph","text":"<p>This is the standard use case for RAG pipelines where document structure (Header -&gt; Body) is preserved.</p> <pre><code>from sayou.assembler.pipeline import AssemblerPipeline\n\npipeline = AssemblerPipeline(strategy=\"hierarchy\")\n\n# Input must follow the Sayou Standard Schema\ngraph_dict = pipeline.run(wrapper_output_data)\n</code></pre>"},{"location":"library-guides/assembler/#22-adding-custom-strategies-tier-3","title":"2.2. Adding Custom Strategies (Tier 3)","text":"<p>You can implement complex logic, such as linking nodes that share the same keywords.</p> <pre><code>from sayou.assembler.interfaces.base_builder import BaseBuilder\n\nclass SemanticBuilder(BaseBuilder):\n    component_name = \"SemanticBuilder\"\n    SUPPORTED_TYPES = [\"semantic\"]\n\n    def _do_assemble(self, data):\n        # Custom logic to link nodes by similarity...\n        return kg.to_dict()\n</code></pre>"},{"location":"library-guides/assembler/#3-logic-deep-dive-automatic-linking","title":"3. Logic Deep Dive: Automatic Linking","text":"<p>The <code>HierarchyBuilder</code> performs intelligent linking to ensure the graph is traversable in both directions.</p> <p>Input Node:</p> <pre><code>Node A: { id: \"child_1\", relations: { \"hasParent\": [\"parent_1\"] } }\n</code></pre> <p>Assembled Graph:</p> <pre><code>1. Edge Created: child_1 --[hasParent]--&gt; parent_1\n2. Reverse Logic Triggered (internal helper)\n3. Edge Created: parent_1 --[hasChild]--&gt; child_1\n</code></pre> <p>This ensures that when a user searches for the \"Parent Topic,\" the system can easily find all \"Child Contents.\"</p>"},{"location":"library-guides/chunking/","title":"Library Guide: sayou-chunking","text":"<p><code>sayou-chunking</code> is the context-aware splitting library designed to bridge the gap between raw text and Knowledge Graphs.</p> <p>While traditional chunkers blindly slice text by character count, <code>sayou-chunking</code> adopts a \"Structure-First\" philosophy. It uses the document's inherent hierarchy (headers, lists) and syntax (markdown tables, code blocks) to create logical, atomic nodes.</p> <p>Within the <code>sayou-rag</code> pipeline, this library accepts clean text from <code>sayou-refinery</code> and produces structured chunks for <code>sayou-wrapper</code> and <code>sayou-assembler</code>.</p>"},{"location":"library-guides/chunking/#1-core-concepts-architecture","title":"1. Core Concepts &amp; Architecture","text":"<p><code>sayou-chunking</code> uses a robust 4-Tier architecture to ensure both stability and extensibility.</p>"},{"location":"library-guides/chunking/#tier-0-the-engine-core-utilities","title":"Tier 0: The Engine (Core Utilities)","text":"<ul> <li><code>TextSegmenter</code>: The low-level engine responsible for the actual slicing. It implements \"Atomic Protection,\" ensuring that critical patterns (tables, code blocks) are never fragmented, even if they exceed the chunk size.</li> </ul>"},{"location":"library-guides/chunking/#tier-1-interfaces-the-contract","title":"Tier 1: Interfaces (The Contract)","text":"<ul> <li><code>BaseSplitter</code>: Defines the standard input (<code>Document</code> object) and output (<code>List[Chunk]</code>). It handles boilerplate logic like error handling and metadata packaging.</li> </ul>"},{"location":"library-guides/chunking/#tier-2-templates-the-defaults","title":"Tier 2: Templates (The Defaults)","text":"<ul> <li><code>RecursiveSplitter</code>: The standard infantry. Splits text recursively using separators (Paragraph -&gt; Newline -&gt; Sentence -&gt; Word).</li> <li><code>StructureSplitter</code>: A generic architect. Splits documents based on user-defined Regex patterns (e.g., splitting by \"Article 1\", \"Section 2\").</li> <li><code>SemanticSplitter</code>: The intelligent analyst. Detects topic shifts using cosine similarity (currently includes a dummy encoder for testing).</li> </ul>"},{"location":"library-guides/chunking/#tier-3-plugins-specialized","title":"Tier 3: Plugins (Specialized)","text":"<ul> <li><code>MarkdownPlugin</code>: The star feature. It understands Markdown syntax to:<ol> <li>Protect tables and code blocks.</li> <li>Classify chunks (<code>h1</code>, <code>table</code>, <code>list_item</code>).</li> <li>Link content to headers via <code>parent_id</code> for Knowledge Graphs.</li> </ol> </li> </ul>"},{"location":"library-guides/chunking/#tier-4-composites-advanced-strategies","title":"Tier 4: Composites (Advanced Strategies)","text":"<ul> <li><code>ParentDocumentSplitter</code>: The commander. It doesn't split text itself but coordinates a \"Parent Strategy\" (e.g., Markdown) and a \"Child Strategy\" (e.g., Recursive) to implement Small-to-Big Retrieval.</li> </ul>"},{"location":"library-guides/chunking/#2-usage-examples","title":"2. Usage Examples","text":""},{"location":"library-guides/chunking/#21-markdown-splitting-kg-blueprint","title":"2.1. Markdown Splitting (KG Blueprint)","text":"<p>This is the recommended way to process Markdown files for RAG. It identifies headers as parent nodes and lists/tables as child nodes.</p> <pre><code>from sayou.chunking.pipeline import ChunkingPipeline\nfrom sayou.chunking.plugins.markdown_plugin import MarkdownPlugin\n\npipeline = ChunkingPipeline(splitters=[MarkdownPlugin()])\npipeline.initialize()\n\nrequest = {\n    \"type\": \"markdown\",\n    \"content\": \"# Title\\n\\nSome text...\",\n    \"metadata\": {\"id\": \"doc_1\"},\n    \"chunk_size\": 1000\n}\n\nchunks = pipeline.split(request)\n</code></pre>"},{"location":"library-guides/chunking/#22-parent-document-strategy-composite","title":"2.2. Parent Document Strategy (Composite)","text":"<p>Use this when you want to retrieve small chunks but provide large context to the LLM. It combines <code>StructureSplitter</code> (for parents) and <code>RecursiveSplitter</code> (for children).</p> <pre><code>from sayou.chunking.composites.parent_document import ParentDocumentSplitter\n\n# Configure the pipeline\npipeline = ChunkingPipeline(splitters=[ParentDocumentSplitter()])\n\nrequest = {\n    \"type\": \"parent_document\",\n    \"content\": \"...\",\n    \"config\": {\n        # Strategy Injection\n        \"parent_strategy\": \"structure\",\n        \"structure_pattern\": r\"\\[Section \\d+\\]\", \n\n        # Sizes\n        \"parent_chunk_size\": 2000,\n        \"chunk_size\": 400 \n    }\n}\n\nchunks = pipeline.split(request)\n# Output contains chunks with doc_level='parent' and 'child'\n</code></pre>"},{"location":"library-guides/chunking/#3-the-chunk-output-schema","title":"3. The <code>Chunk</code> Output Schema","text":"<p>Every splitter returns a list of <code>Chunk</code> objects (serialized to Dicts). This schema is designed to be directly consumable by <code>sayou-assembler</code>.</p> <pre><code>{\n    \"chunk_content\": \"string (The actual text)\",\n    \"metadata\": {\n    \"chunk_id\": \"string (Unique ID, e.g., 'doc_1_part_5')\",\n    \"part_index\": \"int (Order in document)\",\n\n    // --- Semantic Classification (MarkdownPlugin) ---\n    \"semantic_type\": \"Literal['text', 'table', 'code_block', 'h1'...'h6', 'list_item']\",\n\n    // --- Knowledge Graph Linkage ---\n    \"parent_id\": \"Optional[string] (ID of the header or parent chunk)\",\n    \"section_title\": \"Optional[string] (Text of the parent header)\",\n    \"doc_level\": \"Literal['parent', 'child'] (For ParentDocument strategy)\",\n    \"child_ids\": \"List[string] (For Parent nodes to find children)\"\n    }\n}\n</code></pre>"},{"location":"library-guides/chunking/#4-deep-dive-how-protection-works","title":"4. Deep Dive: How Protection Works","text":"<p>One of the biggest challenges in RAG is broken tables. <code>sayou-chunking</code> solves this at the engine level (Tier 0).</p> <ol> <li>Registration: Tier 3 plugins (like <code>MarkdownPlugin</code>) register <code>PROTECTED_PATTERNS</code> (Regex for tables/code).</li> <li>Isolation: <code>TextSegmenter</code> extracts these blocks before any splitting occurs.</li> <li>Atomic Handling:<ul> <li>If a table is 500 chars and <code>chunk_size</code> is 200, it is NOT split. It remains as a single 500-char chunk.</li> <li>This ensures the LLM receives the full table context.</li> </ul> </li> </ol>"},{"location":"library-guides/chunking/#5-extending-tier-3-guide","title":"5. Extending (Tier 3 Guide)","text":"<p>To support a new format (e.g., HTML), you should inherit from <code>RecursiveSplitter</code> and inject your own patterns.</p> <p>from sayou.chunking.splitter.recursive import RecursiveSplitter from sayou.chunking.utils.schema import Document, Chunk</p> <pre><code>class HtmlPlugin(RecursiveSplitter):\n    component_name = \"HtmlPlugin\"\n    SUPPORTED_TYPES = [\"html\"]\n\n    # 1. Define Protection (e.g., &lt;table&gt;...&lt;/table&gt;)\n    PROTECTED_PATTERNS = [r\"(?s)&lt;table.*?&lt;/table&gt;\", r\"(?s)&lt;pre&gt;.*?&lt;/pre&gt;\"]\n\n    # 2. Define Separators (Tags)\n    HTML_SEPARATORS = [r\"&lt;div.*?&gt;\", r\"&lt;p&gt;\", \"\\n\"]\n\n    def _do_split(self, doc: Document) -&gt; List[Chunk]:\n        # Inject configs\n        doc.metadata.setdefault(\"config\", {})\n        doc.metadata[\"config\"][\"separators\"] = self.HTML_SEPARATORS\n        doc.metadata[\"config\"][\"protected_patterns\"] = self.PROTECTED_PATTERNS\n\n        return super()._do_split(doc)\n</code></pre>"},{"location":"library-guides/connector/","title":"Library Guide: sayou-connector","text":"<p><code>sayou-connector</code> serves as the entry point of the RAG pipeline. Its mission is to bring external data into the Sayou ecosystem, regardless of where it lives or how it is structured.</p> <p>It acts as a Recursive Harvesting Machine, capable of discovering data dynamically rather than just reading a static list.</p>"},{"location":"library-guides/connector/#1-core-concepts-architecture","title":"1. Core Concepts &amp; Architecture","text":"<p>The library uses a Generator-Fetcher Pattern to handle complex collection logic.</p>"},{"location":"library-guides/connector/#tier-1-interfaces","title":"Tier 1: Interfaces","text":"<ul> <li><code>BaseGenerator</code>: The brain. It implements the <code>generate()</code> method (Iterator) to produce <code>FetchTask</code>s. It also receives <code>feedback()</code> from Fetchers to decide the next move (e.g., adding discovered URLs to the queue).</li> <li><code>BaseFetcher</code>: The hands. It implements the <code>fetch()</code> method to execute a <code>FetchTask</code> and return a <code>FetchResult</code>.</li> </ul>"},{"location":"library-guides/connector/#tier-2-templates-strategies","title":"Tier 2: Templates (Strategies)","text":"<ul> <li>Generators:<ul> <li><code>PathWalkerGenerator</code>: Recursively scans local directories.</li> <li><code>WebCrawlGenerator</code>: Manages a URL queue and filters links via Regex.</li> <li><code>SqlGenerator</code>: Manages SQL <code>LIMIT</code> and <code>OFFSET</code> for pagination.</li> </ul> </li> <li>Fetchers:<ul> <li><code>FileFetcher</code>: Reads binary/text files.</li> <li><code>SimpleWebFetcher</code>: Uses <code>requests</code> and <code>BeautifulSoup</code> to scrape content.</li> <li><code>SqliteFetcher</code>: Executes queries against SQLite databases.</li> </ul> </li> </ul>"},{"location":"library-guides/connector/#2-usage-examples","title":"2. Usage Examples","text":""},{"location":"library-guides/connector/#21-local-file-scanning","title":"2.1. Local File Scanning","text":"<p>Ideal for ingesting a folder of PDFs or Markdown files.</p> <pre><code>pipeline.run(\n    source=\"./documents\",\n    strategy=\"local_scan\",\n    recursive=True,\n    extensions=[\".pdf\", \".docx\"],\n    name_pattern=\"*report*\"\n)\n</code></pre>"},{"location":"library-guides/connector/#22-database-ingestion-rdb","title":"2.2. Database Ingestion (RDB)","text":"<p>Ideal for pulling structured data from legacy systems.</p> <pre><code>pipeline.run(\n    source=\"my_database.db\", # Connection String\n    strategy=\"sql_scan\",\n    query=\"SELECT * FROM users WHERE active=1\",\n    batch_size=1000\n)\n</code></pre>"},{"location":"library-guides/connector/#23-smart-web-crawling","title":"2.3. Smart Web Crawling","text":"<p>Crawls a website, following links that match specific patterns.</p> <pre><code>pipeline.run(\n    source=\"https://example.com/blog\",\n    strategy=\"web_crawl\",\n    link_pattern=r\"/blog/post/\\d+\",\n    selectors={\"title\": \"h1\", \"body\": \"div.content\"},\n    max_depth=2\n)\n</code></pre>"},{"location":"library-guides/connector/#3-logic-deep-dive-the-feedback-loop","title":"3. Logic Deep Dive: The Feedback Loop","text":"<p>The power of <code>sayou-connector</code> lies in its feedback loop.</p> <ol> <li>Generate: Generator creates a Task (e.g., Fetch <code>Page 1</code>).</li> <li>Fetch: Fetcher downloads <code>Page 1</code> and extracts links (<code>Page 2</code>, <code>Page 3</code>).</li> <li>Feedback: Fetcher passes these links back to the Generator via <code>result.data</code>.</li> <li>Update: Generator adds new links to its internal Queue.</li> <li>Repeat: The loop continues until the Queue is empty or <code>max_depth</code> is reached.</li> </ol> <p>This allows the pipeline to handle dynamic data sources without hardcoding every single target.</p>"},{"location":"library-guides/document/","title":"Library Guide: sayou-document","text":"<p><code>sayou-document</code> is the high-fidelity document extraction library that powers the first step of the <code>sayou-rag</code> pipeline.</p> <p>The library's core philosophy is \"Precise Extraction.\" It focuses on capturing the structural facts of a document\u2014such as fonts, styles, coordinates, and layout\u2014and structuring them without loss of fidelity. This rich, reliable data provides a robust foundation for intelligent, rule-based processing by downstream components like <code>sayou-refinery</code>.</p> <p>Within <code>sayou-rag</code>, this library serves as the default plugin for the 'Extractor' slot in the <code>AdvancedRAG</code> pipeline. It can also be used as a powerful standalone library.</p>"},{"location":"library-guides/document/#1-independent-installation","title":"1. Independent Installation","text":"<p>To use <code>sayou-document</code> independently of the main <code>sayou-rag</code> package, you can install it directly:</p> <pre><code>pip install sayou-document\n</code></pre>"},{"location":"library-guides/document/#2-core-concepts-architecture","title":"2. Core Concepts &amp; Architecture","text":"<p><code>sayou-document</code> is built on a 3-Tier architecture, ensuring high extensibility.</p>"},{"location":"library-guides/document/#tier-1-interfaces-the-contract","title":"Tier 1: Interfaces (The Contract)","text":"<p>Abstract base classes that define the component contracts.</p> <ul> <li><code>BaseDocumentParser</code>: Defines the <code>.parse()</code> method.</li> <li><code>BaseOCR</code>: Defines the <code>.ocr_bytes()</code> method.</li> <li><code>BaseConverter</code>: Defines the <code>.convert()</code> method.</li> </ul>"},{"location":"library-guides/document/#tier-2-templates-the-defaults","title":"Tier 2: Templates (The Defaults)","text":"<p>The default, \"batteries-included\" implementations of the interfaces.</p> <ul> <li><code>PdfParser</code>: Based on <code>fitz(PyMuPDF)</code>. Supports TOC, scanned PDF detection, and image OCR.</li> <li><code>DocxParser</code>: Based on <code>python-docx</code>. Supports headers, footers, and style extraction.</li> <li><code>PptxParser</code>: Based on <code>python-pptx</code>. Supports charts, slide notes, and <code>placeholder_type</code> extraction.</li> <li><code>ExcelParser</code>: Based on <code>openpyxl</code>. Supports hidden sheets, sheet names, and image extraction.</li> </ul>"},{"location":"library-guides/document/#tier-3-plugins-the-customization","title":"Tier 3: Plugins (The Customization)","text":"<p>Any user-implemented component that adheres to a Tier 1 interface. (e.g., <code>GoogleVisionOCR</code>, <code>HwpConverter</code>)</p>"},{"location":"library-guides/document/#3-usage-examples","title":"3. Usage Examples","text":""},{"location":"library-guides/document/#basic-usage-independent","title":"Basic Usage (Independent)","text":"<p>The <code>DocumentPipeline</code> automatically loads Tier 2 parsers and routes files by extension.</p> <pre><code>import os\nfrom sayou.document.pipeline import DocumentPipeline\n\n# 1. Initialize the pipeline (Defaults are loaded)\npipeline = DocumentPipeline()\npipeline.initialize()\n\n# 2. Load file bytes\nfile_path = \"path/to/your/financial_report.pdf\"\nfile_name = os.path.basename(file_path)\n\nwith open(file_path, \"rb\") as f:\n    file_bytes = f.read()\n\n# 3. Run extraction\ndoc = pipeline.run(file_bytes, file_name)\n\n# 4. Export to JSON\njson_output = doc.model_dump_json(indent=2)\nprint(json_output)\n</code></pre>"},{"location":"library-guides/document/#advanced-usage-injecting-plugins","title":"Advanced Usage (Injecting Plugins)","text":"<p>The true power of <code>sayou-document</code> lies in dependency injection. You can provide Tier 3 plugins to the <code>DocumentPipeline</code> constructor to extend or replace default behaviors.</p> <pre><code>from sayou.document.pipeline import DocumentPipeline\n\nfrom my_plugins.ocr import MyCustomTesseractOCR\nfrom my_plugins.converters import MyHwpConverter\n\nmy_ocr = MyCustomTesseractOCR(tesseract_path=\"/usr/bin/tesseract\")\n\nmy_converter = MyHwpConverter(libreoffice_path=\"/usr/bin/soffice\")\n\npipeline = DocumentPipeline(\n    ocr_engine=my_ocr,\n    converter=my_converter\n)\npipeline.initialize()\n\nhwp_doc = pipeline.run(hwp_file_bytes, \"report.hwp\")\n</code></pre>"},{"location":"library-guides/document/#4-the-document-output-schema","title":"4. The <code>Document</code> Output Schema","text":"<p>All parsers return a single, consistent Pydantic model.</p>"},{"location":"library-guides/document/#41-root-document","title":"4.1. Root (<code>Document</code>)","text":"<p>The top-level container for the entire document.</p> <pre><code>{\n  \"file_name\": \"string\",\n  \"file_id\": \"string\",\n  \"doc_type\": \"Literal['pdf', 'word', 'slide', 'sheet', 'unknown']\",\n  \"metadata\": {\n    \"title\": \"Optional[string]\",\n    \"author\": \"Optional[string]\",\n    // ...\n  },\n  \"page_count\": \"int\",\n  \"pages\": \"List[Union[Page, Slide, Sheet]]\",\n  \"toc\": \"List[Dict]\"\n}\n</code></pre>"},{"location":"library-guides/document/#42-containers-page-slide-sheet","title":"4.2. Containers (<code>Page</code>, <code>Slide</code>, <code>Sheet</code>)","text":"<p>The <code>pages</code> list holds containers specific to the document type.</p> <ul> <li><code>Page</code> (PDF, Word):<ul> <li><code>page_num: int</code></li> <li><code>width: float</code>, <code>height: float</code></li> <li><code>elements: List[ElementUnion]</code></li> <li><code>header_elements: List[ElementUnion]</code> (Word)</li> <li><code>footer_elements: List[ElementUnion]</code> (Word)</li> <li><code>text: string</code> (Page text dump)</li> </ul> </li> <li><code>Slide</code> (PPTX):<ul> <li><code>page_num: int</code></li> <li><code>elements: List[ElementUnion]</code></li> <li><code>note_text: Optional[string]</code> (Slide notes)</li> </ul> </li> <li><code>Sheet</code> (XLSX):<ul> <li><code>page_num: int</code> (Sheet index)</li> <li><code>sheet_name: string</code></li> <li><code>is_hidden: bool</code></li> <li><code>elements: List[ElementUnion]</code> (Usually one <code>TableElement</code>)</li> </ul> </li> </ul>"},{"location":"library-guides/document/#43-elements-the-content","title":"4.3. Elements (The Content)","text":"<p><code>ElementUnion = Union[TextElement, TableElement, ImageElement, ChartElement]</code></p> <p>All elements inherit from <code>BaseElement</code> and share:</p> <ul> <li><code>id: string</code> (e.g., \"p1:b3\")</li> <li><code>type: string</code> (\"text\", \"table\", \"image\", \"chart\")</li> <li><code>bbox: BoundingBox</code> (Coordinates)</li> <li><code>meta: ElementMetadata</code> (<code>page_num</code>, etc.)</li> <li><code>raw_attributes: Dict</code>: (High-Fidelity Core) A \"catch-all\" pocket for all other original properties extracted by the parser (e.g., <code>{\"style\": \"Heading 1\"}</code>, <code>{\"placeholder_type\": \"TITLE\"}</code>).</li> </ul> <ul> <li><code>TextElement</code>:<ul> <li><code>text: string</code></li> <li><code>style: Optional[TextStyle]</code> (Font name, size, bold, etc.)</li> </ul> </li> <li><code>TableElement</code>:<ul> <li><code>data: List[List[string]]</code> (A 2D list for LLM-friendly processing)</li> <li><code>cells: List[List[TableCell]]</code> (For structural data, like merged cells. v0.1.0+)</li> </ul> </li> <li><code>ImageElement</code>:<ul> <li><code>image_base64: string</code></li> <li><code>ocr_text: Optional[string]</code> (Result from the OCR engine)</li> </ul> </li> <li><code>ChartElement</code> (PPTX):<ul> <li><code>chart_title: string</code></li> <li><code>chart_type: string</code></li> <li><code>text_representation: string</code> (LLM-friendly text conversion)</li> <li><code>raw_attributes</code>: (Preserves original <code>chart.series</code> data)</li> </ul> </li> </ul>"},{"location":"library-guides/document/#5-tier-2-default-parsers-deep-dive","title":"5. Tier 2 Default Parsers Deep Dive","text":"<p>High-fidelity details provided by the default parsers.</p> <ul> <li><code>PdfParser</code>:<ul> <li>Populates <code>Document.toc</code> from <code>fitz.get_toc()</code>.</li> <li>Automatically detects scanned (image-only) PDFs to perform full-page OCR.</li> <li>Supports OCR for embedded image blocks (<code>b_type == 1</code>).</li> </ul> </li> <li><code>DocxParser</code>:<ul> <li>Populates <code>Page.header_elements</code> and <code>Page.footer_elements</code>.</li> <li>Populates <code>TextElement.raw_attributes[\"style\"]</code> with Word style names (e.g., \"Heading 1\").</li> </ul> </li> <li><code>PptxParser</code>:<ul> <li>Populates <code>Element.raw_attributes[\"placeholder_type\"]</code> (e.g., \"TITLE\", \"BODY\") for semantic layout analysis.</li> <li>Extracts <code>ChartElement</code> data and <code>Slide.note_text</code> (speaker notes).</li> </ul> </li> <li><code>ExcelParser</code>:<ul> <li>Populates <code>Sheet.is_hidden</code> and <code>Sheet.sheet_name</code>.</li> <li>Extracts <code>XLImage</code> (images embedded in sheets) and supports OCR.</li> <li>Roadmap (v0.1.0): <code>TableCell.style</code> (cell formatting, merge info).</li> </ul> </li> </ul>"},{"location":"library-guides/document/#6-extending-tier-3-guide","title":"6. Extending (Tier 3 Guide)","text":"<p>The most common extensions are custom OCR engines or file converters.</p>"},{"location":"library-guides/document/#example-creating-a-custom-ocr-plugin","title":"Example: Creating a Custom OCR Plugin","text":"<pre><code>from sayou.document.interfaces.base_ocr import BaseOCR\nimport my_ocr_library # (\uac00\uc0c1\uc758 \ub77c\uc774\ube0c\ub7ec\ub9ac)\n\nclass MyAwesomeOCR(BaseOCR):\n    \"\"\"(Tier 3) MyAwesomeOCR\uc758 \uc778\ud130\ud398\uc774\uc2a4 \uad6c\ud604\uccb4\"\"\"\n\n    component_name = \"MyAwesomeOCR\"\n\n    def __init__(self, api_key: str):\n        super().__init__()\n        self.client = my_ocr_library.Client(api_key=api_key)\n        self._log(f\"MyAwesomeOCR initialized.\")\n\n    @abstractmethod\n    def _do_ocr(self, image_bytes: bytes, **kwargs) -&gt; str:\n        \"\"\"\n        [\uad6c\ud604 \ud544\uc218] BaseOCR\uc758 \ucd94\uc0c1 \uba54\uc11c\ub4dc \uad6c\ud604.\n        \uc2e4\uc81c OCR \ub85c\uc9c1\uc740 \uc5ec\uae30\uc11c \ucc98\ub9ac\ud569\ub2c8\ub2e4.\n        \"\"\"\n        try:\n            # BaseOCR.ocr_bytes()\uac00 \uc5d0\ub7ec \ud578\ub4e4\ub9c1\uc744 \ud574\uc8fc\ubbc0\ub85c,\n            # \uc5ec\uae30\uc11c\ub294 \uc131\uacf5 \uc2dc \ud14d\uc2a4\ud2b8 \ubc18\ud658\uc5d0\ub9cc \uc9d1\uc911\ud569\ub2c8\ub2e4.\n            results = self.client.process(image=image_bytes)\n            return results.text\n        except Exception as e:\n            self._log(f\"MyAwesomeOCR failed: {e}\", level=\"error\")\n            return \"\" # \uc2e4\ud328 \uc2dc \ube48 \ubb38\uc790\uc5f4 \ubc18\ud658\n</code></pre> <pre><code>from sayou.document.pipeline import DocumentPipeline\n\nocr_plugin = MyAwesomeOCR(api_key=\"MY_SECRET_KEY\")\npipeline = DocumentPipeline(ocr_engine=ocr_plugin)\n\npipeline.run(...)\n</code></pre>"},{"location":"library-guides/extractor/","title":"sayou-extractor","text":"<p><code>sayou-extractor</code> provides standardized interfaces for retrieving, querying, and searching for data from various sources like files, SQL databases, and vector stores.</p>"},{"location":"library-guides/extractor/#1-concepts-core-interfaces","title":"1. Concepts (Core Interfaces)","text":"<p>This library provides three distinct T1 Interfaces (verbs) for data extraction, all managed by the <code>ExtractorPipeline</code>.</p>"},{"location":"library-guides/extractor/#baseretriever-t1","title":"<code>BaseRetriever</code> (T1)","text":"<p>This interface defines the contract for Key-Value or File-based lookups (e.g., \"get this specific file\"). It uses a <code>retrieve()</code> method.</p>"},{"location":"library-guides/extractor/#basequerier-t1","title":"<code>BaseQuerier</code> (T1)","text":"<p>This interface defines the contract for structured queries, such as SQL or Cypher (e.g., \"SELECT * FROM users WHERE...\"). It uses a <code>query()</code> method.</p>"},{"location":"library-guides/extractor/#basesearcher-t1","title":"<code>BaseSearcher</code> (T1)","text":"<p>This interface defines the contract for similarity search (e.g., vector KNN). It uses a <code>search()</code> method.</p>"},{"location":"library-guides/extractor/#2-t2-usage-default-components","title":"2. T2 Usage (Default Components)","text":"<p>You use <code>sayou-extractor</code> by initializing the <code>ExtractorPipeline</code> and passing T2 components to the corresponding \"verb\" list.</p>"},{"location":"library-guides/extractor/#using-retrievers-fileretriever-t2","title":"Using Retrievers (<code>FileRetriever</code>, T2)","text":"<p>(Placeholder for text explaining that <code>FileRetriever</code> is the default T2 for <code>retrieve()</code> calls. It reads from a local file system.)</p> <p>(Placeholder for a code-free explanation: \"You pass an instance of <code>FileRetriever</code> to the <code>retrievers=[]</code> argument of the <code>ExtractorPipeline</code>.)</p>"},{"location":"library-guides/extractor/#using-queriers-sqlquerier-t2","title":"Using Queriers (<code>SqlQuerier</code>, T2)","text":"<p>(Placeholder for text explaining <code>SqlQuerier</code> is the T2 for <code>query()</code> calls. It uses SQLAlchemy to run standard SQL against a configured DB.)</p> <p>(Placeholder for a code-free explanation: \"You pass <code>SqlQuerier</code> to the <code>queriers=[]</code> argument. The pipeline will route any request with <code>type: 'sql'</code> to it.\")</p>"},{"location":"library-guides/extractor/#using-searchers-vectorsearchtemplate-t2","title":"Using Searchers (<code>VectorSearchTemplate</code>, T2)","text":"<p>(Placeholder for text explaining <code>VectorSearchTemplate</code> is an abstract T2, and that T3 plugins (like <code>PgvectorSearcher</code>) are the concrete implementations.)</p>"},{"location":"library-guides/extractor/#3-t3-plugin-development","title":"3. T3 Plugin Development","text":"<p>A common use case is adding a T3 plugin to query a specific database, like Neo4j.</p>"},{"location":"library-guides/extractor/#tutorial-building-a-neo4jcypherquerier-t3","title":"Tutorial: Building a <code>Neo4jCypherQuerier</code> (T3)","text":"<p>(Placeholder for a step-by-step text tutorial.) 1.  Create your class: Define <code>Neo4jCypherQuerier</code> in the <code>plugins/neo4j/</code> folder. 2.  Inherit T1: Make your class inherit from <code>BaseQuerier</code> (T1). 3.  Implement <code>_do_query</code>: Inside this method, you will write the logic to connect to Neo4j and execute the Cypher query. 4.  Use it: You pass your new <code>Neo4jCypherQuerier</code> to the <code>queriers=[]</code> list in the <code>ExtractorPipeline</code>. The pipeline will now route requests with <code>type: 'cypher'</code> (which you define) to your T3 plugin.</p>"},{"location":"library-guides/extractor/#4-api-reference","title":"4. API Reference","text":""},{"location":"library-guides/extractor/#tier-1-interfaces","title":"Tier 1: Interfaces","text":"Interface File Description <code>BaseRetriever</code> <code>interfaces/base_retriever.py</code> Contract for K-V/File retrieval. <code>BaseQuerier</code> <code>interfaces/base_querier.py</code> Contract for structured (SQL) queries. <code>BaseSearcher</code> <code>interfaces/base_searcher.py</code> Contract for similarity search."},{"location":"library-guides/extractor/#tier-2-default-components","title":"Tier 2: Default Components","text":"Component Directory Implements <code>FileRetriever</code> <code>retriever/</code> <code>BaseRetriever</code> <code>SqlQuerier</code> <code>querier/</code> <code>BaseQuerier</code> <code>VectorSearchTemplate</code> <code>searcher/</code> <code>BaseSearcher</code>"},{"location":"library-guides/extractor/#tier-3-official-plugins","title":"Tier 3: Official Plugins","text":"Plugin Directory Implements <code>PgvectorSearcher</code> <code>plugins/</code> <code>VectorSearchTemplate</code> (T2) <code>CypherQuerier</code> <code>plugins/neo4j/</code> <code>BaseQuerier</code> (T1)"},{"location":"library-guides/llm/","title":"sayou-llm","text":"<p><code>sayou-llm</code> is the universal LLM Adapter for the Sayou Data Platform. It provides a single, standardized interface (<code>BaseLLMClient</code>) to interact with any LLM, whether it's a remote API (like OpenAI) or a local model (like GGUF or Transformers).</p>"},{"location":"library-guides/llm/#1-concepts-core-interfaces","title":"1. Concepts (Core Interfaces)","text":""},{"location":"library-guides/llm/#basellmclient-t1","title":"<code>BaseLLMClient</code> (T1)","text":"<p>This is the single \"socket\" for all LLMs. It defines the standard <code>invoke()</code> and <code>stream()</code> methods that the entire <code>sayou-rag</code> library relies on. By conforming to this T1 interface, any LLM can be \"plugged into\" the RAG system.</p>"},{"location":"library-guides/llm/#2-t2-usage-default-adapters","title":"2. T2 Usage (Default Adapters)","text":"<p>T2 components are the \"official\" adapters for popular LLM providers and local formats, found in the <code>llm_client/</code> directory.</p>"},{"location":"library-guides/llm/#using-openaiclient-t2-remote-api","title":"Using <code>OpenAIClient</code> (T2 - Remote API)","text":"<p>(Placeholder for text explaining this T2 component connects to the OpenAI API (or Azure). It requires an <code>OPENAI_API_KEY</code>.)</p>"},{"location":"library-guides/llm/#using-llamacppclient-t2-local-load","title":"Using <code>LlamaCppClient</code> (T2 - Local Load)","text":"<p>(Placeholder for text explaining this T2 component loads <code>GGUF</code> formatted models directly into memory using <code>llama-cpp-python</code>. It requires the <code>sayou-llm[gguf]</code> extra.)</p>"},{"location":"library-guides/llm/#using-transformersclient-t2-local-load","title":"Using <code>TransformersClient</code> (T2 - Local Load)","text":"<p>(Placeholder for text explaining this T2 component loads original Hugging Face <code>safetensors</code> models (like Gemma, Llama 3) directly into memory using the <code>transformers</code> library. It requires the <code>sayou-llm[transformers]</code> extra.)</p>"},{"location":"library-guides/llm/#3-t3-plugin-development","title":"3. T3 Plugin Development","text":"<p>A T3 plugin can either wrap a T2 adapter to add functionality (like logging) or be a custom adapter for a non-supported model.</p>"},{"location":"library-guides/llm/#tutorial-building-a-loggingllmclient-t3","title":"Tutorial: Building a <code>LoggingLLMClient</code> (T3)","text":"<p>(Placeholder for a step-by-step text tutorial.) 1.  Create your class: Define <code>LoggingLLMClient</code>. 2.  Inherit T2: Make your class inherit from <code>OpenAIClient</code> (T2). 3.  Override <code>invoke</code>: Override the <code>invoke</code> method. Add your logging logic (e.g., start timer), then call <code>super().invoke()</code> to run the original T2 logic, and finally log the duration. 4.  Use it: Pass your <code>LoggingLLMClient</code> instance (instead of the <code>OpenAIClient</code>) to the <code>sayou-rag</code> generator.</p>"},{"location":"library-guides/llm/#tutorial-building-an-hfnativeclient-t3","title":"Tutorial: Building an <code>HFNativeClient</code> (T3)","text":"<p>(Placeholder for text explaining this (from <code>plugins/hf_native_client.py</code>) is an advanced T3 plugin that inherits T1 directly to provide fine-grained control over HF model generation, including custom chat templates.)</p>"},{"location":"library-guides/llm/#4-api-reference","title":"4. API Reference","text":""},{"location":"library-guides/llm/#tier-1-interfaces","title":"Tier 1: Interfaces","text":"Interface File Description <code>BaseLLMClient</code> <code>interfaces/base_llm_client.py</code> The single contract for all LLM interactions."},{"location":"library-guides/llm/#tier-2-default-components","title":"Tier 2: Default Components","text":"Component Directory Description <code>OpenAIClient</code> <code>llm_client/</code> Connects to OpenAI/Azure API. <code>AnthropicClient</code> <code>llm_client/</code> Connects to Anthropic Claude API. <code>GeminiClient</code> <code>llm_client/</code> Connects to Google Gemini API. <code>OllamaClient</code> <code>llm_client/</code> Connects to a local Ollama server API. <code>LlamaCppClient</code> <code>llm_client/</code> Loads GGUF models directly. <code>TransformersClient</code> <code>llm_client/</code> Loads HF <code>safetensors</code> models directly."},{"location":"library-guides/llm/#tier-3-official-plugins","title":"Tier 3: Official Plugins","text":"Plugin Directory Implements/Wraps <code>HFNativeClient</code> <code>plugins/</code> <code>BaseLLMClient</code> (T1) <code>LoggingLLMClient</code> <code>plugins/</code> <code>OpenAIClient</code> (T2)"},{"location":"library-guides/loader/","title":"Library Guide: sayou-loader","text":"<p><code>sayou-loader</code> manages the egress of data from the pipeline. It connects the ephemeral processing world (Memory) to the persistent storage world (Disk/DB).</p> <p>It is designed to be the final, reliable step in the <code>sayou-rag</code> ingestion process.</p>"},{"location":"library-guides/loader/#1-core-concepts-architecture","title":"1. Core Concepts &amp; Architecture","text":"<p>The library follows the standard 3-Tier Architecture with a focus on fail-safety.</p>"},{"location":"library-guides/loader/#tier-1-interfaces","title":"Tier 1: Interfaces","text":"<ul> <li><code>BaseLoader</code>: The abstract base class. It employs the Template Method Pattern, implementing standardized logging and error handling in the <code>load()</code> method while delegating actual logic to <code>_do_load()</code>.</li> </ul>"},{"location":"library-guides/loader/#tier-2-templates-the-defaults","title":"Tier 2: Templates (The Defaults)","text":"<ul> <li><code>FileLoader</code>: The workhorse. It saves data to the local filesystem. It intelligently handles different data types:<ul> <li><code>Dict/List</code> -&gt; JSON file</li> <li><code>Bytes</code> -&gt; Binary file</li> <li><code>String</code> -&gt; Text file</li> <li><code>Other</code> -&gt; Pickle file</li> </ul> </li> </ul>"},{"location":"library-guides/loader/#orchestrator-pipeline-fallback","title":"Orchestrator: Pipeline &amp; Fallback","text":"<p>The <code>LoaderPipeline</code> contains critical logic: Smart Fallback.</p> <pre><code>if handler_not_found:\n    log_warning(\"Fallback to FileLoader\")\n    handler = FileLoader()\n</code></pre> <p>This ensures that long-running pipeline jobs do not end in data loss due to a misconfiguration or network issue at the very last step.</p>"},{"location":"library-guides/loader/#2-usage-examples","title":"2. Usage Examples","text":""},{"location":"library-guides/loader/#21-saving-knowledge-graphs-standard-rag","title":"2.1. Saving Knowledge Graphs (Standard RAG)","text":"<p>In a typical <code>sayou-rag</code> workflow, the Assembler outputs a dictionary representing the graph.</p> <pre><code>loader.run(\n    data=kg_graph_dict, \n    destination=\"data/kg_v1.json\", \n    target_type=\"file\"\n)\n</code></pre>"},{"location":"library-guides/loader/#22-integrating-with-graph-databases-tier-3","title":"2.2. Integrating with Graph Databases (Tier 3)","text":"<p>You can extend <code>sayou-loader</code> to write directly to databases like Neo4j.</p> <pre><code># (Requires sayou-loader[neo4j] or custom plugin)\nloader.run(\n    data=kg_graph_dict,\n    destination=\"bolt://neo4j:password@localhost:7687\",\n    target_type=\"neo4j\"\n)\n</code></pre>"},{"location":"library-guides/loader/#3-creating-custom-loaders-tier-3","title":"3. Creating Custom Loaders (Tier 3)","text":"<p>To support a new storage backend (e.g., AWS S3), simply inherit from <code>BaseLoader</code>.</p> <pre><code>from sayou.loader.interfaces.base_loader import BaseLoader\n\nclass S3Loader(BaseLoader):\n    component_name = \"S3Loader\"\n    SUPPORTED_TYPES = [\"s3\"]\n\n    def _do_load(self, data, destination, **kwargs):\n        # Logic to upload data to S3 bucket (destination)\n        s3_client.put_object(...)\n        return True\n</code></pre> <p>Register this loader with the pipeline using the <code>extra_loaders</code> argument during initialization.</p>"},{"location":"library-guides/overview/","title":"Overview","text":"<p>Sayou Data Platform adopts a 3-Tier interface architecture to ensure modularity, extensibility, and maintainability.</p>"},{"location":"library-guides/overview/#1-system-overview","title":"1. System Overview","text":"<p>(diagram here)</p> <p>Each module defines its own isolated data flow, built around a consistent set of base contracts and execution patterns.</p>"},{"location":"library-guides/overview/#2-3-tier-architecture","title":"2. 3-Tier Architecture","text":"Tier Role Description 1 Interface Defines contracts Abstract base classes such as <code>BaseFetcher</code>, <code>BaseChunker</code>, <code>BaseLLMClient</code>. 2 Default Provides standard implementations Core utilities and default strategies ready for production use. 3 Plugin Enables user-defined extensions Custom connectors, retrieval logic, or LLM adapters integrated via T1 interface. <p>This structure ensures every module can be replaced or extended without breaking the overall pipeline.</p>"},{"location":"library-guides/overview/#3-data-flow-composition","title":"3. Data Flow Composition","text":"<p>(diagram here \u2014 conceptual data flow)</p> <ol> <li>Ingestion (Connector) \u2013 Fetch raw data from APIs, files, or databases.</li> <li>Preprocessing (Wrapper, Chunking, Refinery) \u2013 Normalize, segment, and refine data.</li> <li>Persistence (Assembler, Loader) \u2013 Structure and store data into vector or relational stores.</li> <li>Retrieval (Extractor) \u2013 Query and fetch relevant data.</li> <li>Generation (LLM / RAG) \u2013 Use the retrieved context to generate results.</li> </ol>"},{"location":"library-guides/overview/#4-execution-model","title":"4. Execution Model","text":"<p>Each stage is executed explicitly through the <code>RAGExecutor</code> or user-defined pipelines.</p> <pre><code>from sayou.connector import ApiFetcher\nfrom sayou.refinery import RefineryPipeline\nfrom sayou.rag import RAGExecutor\n\nfetcher = ApiFetcher(base_url=\"https://api.example.com\")\ndata = fetcher.fetch({\"endpoint\": \"/news/latest\"})\nrefined = RefineryPipeline().process(data)\n\nresult = RAGExecutor().run(\"Summarize today's news\", context=refined)\nprint(result)\n</code></pre> <p>This model supports fine-grained debugging and unit testing across all pipeline nodes.</p>"},{"location":"library-guides/overview/#5-scalability-and-deployment","title":"5. Scalability and Deployment","text":"<ul> <li>Modular packages enable microservice-style deployment.</li> <li>Each component can run independently or as part of a unified workflow.</li> <li>Supports both local and distributed execution contexts.</li> </ul>"},{"location":"library-guides/rag/","title":"sayou-rag","text":"<p><code>sayou-rag</code> is the Orchestration Engine of the Sayou Data Platform. It does not provide a single pipeline, but rather an <code>RAGExecutor</code> that runs a \"graph\" of composable \"nodes\" (components) to execute complex RAG and Agentic workflows.</p>"},{"location":"library-guides/rag/#1-concepts-core-interfaces","title":"1. Concepts (Core Interfaces)","text":"<p><code>sayou-rag</code> defines the T1 interfaces for each \"node\" in a RAG workflow.</p> <ul> <li><code>BaseRouter</code> (T1): The contract for classifying a user's query (e.g., \"Is this a billing question or a general question?\").</li> <li><code>BaseTracer</code> (T1): The contract for tracing a \"route\" to a specific data source (e.g., \"Billing questions map to the <code>billing_db</code>\").</li> <li><code>BaseTransformer</code> (T1): The contract for transforming the user's query (e.g., HyDE, Sub-query generation).</li> <li><code>BaseFetcher</code> (T1): The contract for retrieving context from a source (e.g., <code>sayou-extractor</code>).</li> <li><code>BaseGenerator</code> (T1): The contract for generating a final response using the context (e.g., <code>sayou-llm</code>).</li> </ul>"},{"location":"library-guides/rag/#2-t2-usage-assembling-a-pipeline","title":"2. T2 Usage (Assembling a Pipeline)","text":"<p>You do not \"use\" <code>sayou-rag</code>. You \"assemble\" it. The T2 components (in <code>router/</code>, <code>fetcher/</code>, etc.) are \"nodes\" that you give to the <code>RAGExecutor</code>.</p>"},{"location":"library-guides/rag/#example-assembling-an-agentic-rag","title":"Example: Assembling an Agentic RAG","text":"<p>(Placeholder for a text-based explanation of the E2E example.) 1.  Instantiate Tools: Create instances of your tools (e.g., <code>sayou-extractor</code> and <code>sayou-llm</code>). 2.  Instantiate T2 Nodes:     * <code>SayouSftRouter</code> (T2) is given the <code>sayou-llm</code> client for routing.     * <code>KgTracer</code> (T2) is given the <code>sayou-extractor</code> for KG lookups.     * <code>VectorFetcher</code> (T2) is given the <code>sayou-extractor</code> for vector search.     * <code>LlmGenerator</code> (T2) is given the <code>sayou-llm</code> client for generation. 3.  Instantiate Executor: Pass all four T2 nodes to the <code>RAGExecutor</code>. 4.  Run: Call <code>executor.run(query)</code>. The executor will automatically run the nodes in the correct order (Router -&gt; Tracer -&gt; Fetcher -&gt; Generator).</p>"},{"location":"library-guides/rag/#example-assembling-a-simple-rag","title":"Example: Assembling a Simple RAG","text":"<p>(Placeholder for text explaining that if you only pass a <code>Fetcher</code> and a <code>Generator</code> to the <code>RAGExecutor</code>, it will automatically skip the (Router/Tracer) steps and run a simpler pipeline.)</p>"},{"location":"library-guides/rag/#3-t3-plugin-development","title":"3. T3 Plugin Development","text":"<p>A T3 plugin in <code>sayou-rag</code> is a custom \"node\" in the graph, such as a Reranker.</p>"},{"location":"library-guides/rag/#tutorial-building-a-coherereranker-t3","title":"Tutorial: Building a <code>CohereReranker</code> (T3)","text":"<p>(Placeholder for a step-by-step text tutorial.) 1.  Create your class: Define <code>CohereRerankerPlugin</code>. 2.  Inherit T1? No, a Reranker is a special step. It might wrap a <code>BaseFetcher</code> (T1). 3.  Wrap <code>fetch</code>: Create a T3 class <code>RerankingFetcher(BaseFetcher)</code>. 4.  Add Logic: Its <code>__init__</code> takes a T2 <code>VectorFetcher</code> and a Cohere client. Its <code>_do_fetch</code> method first calls the T2 <code>VectorFetcher</code> to get 50 documents, then passes them to Cohere's Rerank API, and finally returns only the top 5. 5.  Use it: Pass your <code>RerankingFetcher</code> (T3) to the <code>fetcher=</code> argument of the <code>RAGExecutor</code>.</p>"},{"location":"library-guides/rag/#4-api-reference","title":"4. API Reference","text":""},{"location":"library-guides/rag/#tier-1-interfaces","title":"Tier 1: Interfaces","text":"Interface File Description <code>BaseRouter</code> <code>interfaces/base_router.py</code> Contract for query classification. <code>BaseTracer</code> <code>interfaces/base_tracer.py</code> Contract for mapping routes to sources. <code>BaseTransformer</code> <code>interfaces/base_transformer.py</code> Contract for query transformation. <code>BaseFetcher</code> <code>interfaces/base_fetcher.py</code> Contract for context retrieval. <code>BaseGenerator</code> <code>interfaces/base_generator.py</code> Contract for final response generation."},{"location":"library-guides/rag/#tier-2-default-components","title":"Tier 2: Default Components","text":"Component Directory Implements <code>SftRouter</code> <code>router/</code> <code>BaseRouter</code> <code>KgTracer</code> <code>tracer/</code> <code>BaseTracer</code> <code>HydeTransformer</code> <code>transformer/</code> <code>BaseTransformer</code> <code>VectorFetcher</code> <code>fetcher/</code> <code>BaseFetcher</code> <code>LlmGenerator</code> <code>generator/</code> <code>BaseGenerator</code>"},{"location":"library-guides/rag/#tier-3-official-plugins","title":"Tier 3: Official Plugins","text":"Plugin Directory Implements/Wraps <code>CohereReranker</code> <code>plugins/</code> Wraps <code>BaseFetcher</code> (T1)"},{"location":"library-guides/refinery/","title":"Library Guide: sayou-refinery","text":"<p><code>sayou-refinery</code> is the \"smelter\" of the Sayou Data Platform. Its single responsibility is data transformation. It takes raw data structures provided by extractors (<code>document</code>, <code>connector</code>) and transforms them into clean, structured, and intelligent formats for the next stage of the pipeline (<code>chunk</code>, <code>wrapper</code>, <code>rag</code>).</p> <p>This library is crucial for reducing noise and increasing signal before feeding data to an LLM.</p>"},{"location":"library-guides/refinery/#1-independent-installation","title":"1. Independent Installation","text":"<p>To use <code>sayou-refinery</code> independently of the main <code>sayou-rag</code> package:</p> <pre><code>pip install sayou-refinery\n</code></pre>"},{"location":"library-guides/refinery/#2-core-concepts-a-dual-role-smelter","title":"2. Core Concepts: A Dual-Role Smelter","text":"<p><code>sayou-refinery</code> is designed to handle two different types of data, and it provides a separate set of tools for each.</p> <ul> <li> <p>Path A: Document Refining</p> <ul> <li>Input: A single, large <code>Dict</code> (JSON) object from <code>sayou-document</code>.</li> <li>Action: Interprets high-fidelity metadata (<code>raw_attributes</code>) to generate semantic content.</li> <li>Output: A <code>List[SayouBlock]</code> (e.g., Markdown text, image data) ready for <code>sayou-chunk</code>.</li> </ul> </li> <li> <p>Path B: DataAtom Refining</p> <ul> <li>Input: A <code>List[DataAtom]</code> (e.g., from <code>sayou-connector</code> or <code>sayou-wrapper</code>).</li> <li>Action: Cleans, filters, summarizes, or enriches the <code>DataAtom</code>s.</li> <li>Output: A new, refined <code>List[DataAtom]</code> ready for <code>sayou-assembler</code> or <code>sayou-rag</code>.</li> </ul> </li> </ul>"},{"location":"library-guides/refinery/#3-the-sayou-document-contract-input-specification","title":"3. The <code>sayou-document</code> Contract (Input Specification)","text":"<p>The Document Refiner (<code>DocToMarkdownRefiner</code>) is a specialist, not a generalist. It is explicitly designed to understand the output schema of <code>sayou-document</code>.</p> <p>It does not accept arbitrary JSON. It expects a <code>Dict</code> with the following structure:</p> <pre><code>{\n  \"file_name\": \"str (e.g., 'report.docx')\",\n  \"metadata\": { \"title\": \"str\", \"author\": \"str\", \"...\": \"...\" },\n  \"pages\": [\n    {\n      \"page_num\": \"int\",\n      \"elements\": [\n        {\n          \"type\": \"text\",\n          \"text\": \"str (The actual text content)\",\n          \"raw_attributes\": {\n            \"semantic_type\": \"heading (or 'list', 'normal')\",\n            \"heading_level\": 1,\n            \"list_level\": 0,\n            \"style\": \"str (e.g., 'Heading 1')\"\n          },\n          \"meta\": { \"page_num\": \"int\", \"id\": \"str\" }\n        },\n        {\n          \"type\": \"image\",\n          \"image_base64\": \"str (base64 encoded string)\",\n          \"image_format\": \"str (e.g., 'png')\",\n          \"ocr_text\": \"str (Text from OCR)\",\n          \"meta\": { \"page_num\": \"int\", \"id\": \"str\" }\n        },\n        {\n          \"type\": \"table\",\n          \"data\": [ [\"Header 1\"], [\"Row 1 Cell 1\"] ],\n          \"meta\": { \"page_num\": \"int\", \"id\": \"str\" }\n        },\n        {\n          \"type\": \"chart\",\n          \"text_representation\": \"str (LLM-friendly text of chart data)\",\n          \"meta\": { \"page_num\": \"int\", \"id\": \"str\" }\n        }\n      ],\n      \"footer_elements\": [ \"...\" ]\n    }\n  ]\n}\n</code></pre> <p><code>sayou-refinery</code>'s job is to read these <code>raw_attributes</code> (like <code>semantic_type</code>) and make intelligent decisions (like converting to <code># Heading</code>).</p>"},{"location":"library-guides/refinery/#4-key-components-and-usage","title":"4. Key Components and Usage","text":""},{"location":"library-guides/refinery/#41-document-refiners-tier-1-basedocrefiner","title":"4.1. Document Refiners (Tier 1: <code>BaseDocRefiner</code>)","text":"<p>This component transforms a <code>sayou-document</code> dictionary into <code>SayouBlock</code> objects.</p> <ul> <li>Tier 2 (<code>doc/markdown.py</code>): <code>DocToMarkdownRefiner</code><ul> <li>This is the high-fidelity default engine. It reads the <code>raw_attributes</code> (\"style\", \"semantic_type\", \"placeholder_type\") to generate the richest possible Markdown, including headings (<code>#</code>), lists (<code>-</code>), tables, and image data.</li> </ul> </li> </ul> <pre><code>from sayou.refinery.processor.doc_to_markdown import DocToMarkdownRefiner\nimport json\n\n# 1. Load the JSON dictionary from sayou-document\nwith open(\"my_document_output.json\", \"r\", encoding=\"utf-8\") as f:\n    doc_data = json.load(f)\n\n# 2. Initialize the Tier 2 refiner engine\n# This engine knows how to read the spec above.\nrefiner = DocToMarkdownRefiner()\nrefiner.initialize(include_footers=False) # (Default: False)\n\n# 3. Refine the dict into a list of SayouBlocks\n# This list is the input for 'sayou-chunk'\ncontent_blocks = refiner.refine(doc_data)\n\n# content_blocks[0].type -&gt; \"md\"\n# content_blocks[0].content -&gt; \"--- (frontmatter) ---\"\n# content_blocks[1].type -&gt; \"md\"\n# content_blocks[1].content -&gt; \"# Heading Text\"\n# content_blocks[2].type -&gt; \"image_base64\"\n# content_blocks[2].content -&gt; \"iVBORw0KGgo...\"\n</code></pre> <ul> <li>Output: <code>SayouBlock</code><ul> <li>The <code>refine()</code> method returns a <code>List[SayouBlock]</code>.</li> <li>A <code>SayouBlock</code> is a simple dataclass with <code>type</code> (e.g., \"md\", \"image_base64\"), <code>content</code> (the data), and <code>metadata</code>.</li> <li><code>sayou-chunk</code> consumes this list directly.</li> </ul> </li> </ul>"},{"location":"library-guides/refinery/#42-dataatom-refiners-tier-1-baseprocessor-baseaggregator-basemerger","title":"4.2. DataAtom Refiners (Tier 1: <code>BaseProcessor</code>, <code>BaseAggregator</code>, <code>BaseMerger</code>)","text":"<p>These components operate on <code>List[DataAtom]</code> streams.</p> <ul> <li><code>BaseProcessor</code> (1:1 Transformation)<ul> <li>What it is: A processor that transforms one atom at a time without reference to others.</li> <li>Tier 2 Examples: <code>Deduplicator</code> (removes atoms with duplicate keys), <code>Imputer</code> (fills missing values), <code>TextCleaner</code> (strips HTML from a payload field).</li> </ul> </li> </ul> <pre><code>from sayou.core.atom import DataAtom\nfrom sayou.refinery.core.context import RefineryContext\nfrom sayou.refinery.processor.deduplicator import Deduplicator\n\natoms = [\n    DataAtom(\"source\", \"item\", {\"id\": \"123\", \"data\": \"A\"}),\n    DataAtom(\"source\", \"item\", {\"id\": \"123\", \"data\": \"B_dupe\"})\n]\ncontext = RefineryContext(atoms=atoms)\n\n# Initialize processor with rules\ndeduper = Deduplicator()\ndeduper.initialize(key_field=\"payload.id\")\n\n# Process\nrefined_context = deduper.process(context)\n# len(refined_context.atoms) == 1\n</code></pre> <ul> <li><code>BaseAggregator</code> (N:M Transformation)<ul> <li>What it is: A processor that consumes many atoms and produces one or more new summary atoms. This is critical for reducing noise for LLMs.</li> <li>Tier 2 Examples: <code>AverageAggregator</code> (consumes 30 daily atoms, produces 1 \"monthly average\" atom).</li> </ul> </li> </ul> <pre><code>from sayou.core.atom import DataAtom\nfrom sayou.refinery.core.context import RefineryContext\n# T2 Template\nfrom sayou.refinery.aggregator.average import AverageAggregator \n# T3 Plugin (Hypothetical)\nfrom sayou.refinery.plugins.average import SubwayAverageRefiner \n\n# 1. Prepare 30 daily atoms (example)\natoms = [DataAtom(\"api\", \"ridership\", {\"...\": \"...\"}), ...] # 30 atoms\ncontext = RefineryContext(atoms=atoms)\n\n# 2. Initialize Tier 3 Aggregator (which uses Tier 2 engine)\n# This plugin knows *how* to find keys in 'ridership' atoms\naggregator = SubwayAverageRefiner()\naggregator.initialize()\n\n# 3. Process\n# This consumes 30 atoms and outputs 1 summary atom\nrefined_context = aggregator.process(context)\n# len(refined_context.atoms) == 1\n# refined_context.atoms[0].type == \"refined_average\"\n</code></pre> <ul> <li><code>BaseMerger</code> (N+E:N Enrichment)<ul> <li>What it is: A processor that enriches atoms by looking up information from an external data source (like a <code>dict</code> or database connection) provided in the <code>RefineryContext</code>.</li> <li>Tier 2 Examples: <code>KeyBasedMerger</code> (takes an atom with <code>product_id: 123</code> and merges it with external data to add <code>product_name: \"Widget\"</code>).</li> </ul> </li> </ul>"},{"location":"library-guides/refinery/#5-extending-tier-3-guide","title":"5. Extending (Tier 3 Guide)","text":"<p>Tier 3 plugins are used to override the default Tier 2 behavior.</p> <p>For example, the default <code>DocToMarkdownRefiner</code>(T2) converts <code>semantic_type: \"heading\"</code> to <code># Heading</code>. A Tier 3 plugin (<code>plugins/semantic_html_refiner.py</code>) could inherit from it and override the <code>_handle_text</code> method to convert it to <code>&lt;h1&gt;Heading&lt;/h1&gt;</code> instead.</p> <pre><code>from sayou.refinery.processor.doc_to_markdown import DocToMarkdownRefiner\nfrom sayou.refinery.interfaces.base_doc_refiner import SayouBlock\nfrom typing import Dict, Any, List\n\nclass HtmlRefinerPlugin(DocToMarkdownRefiner):\n    \"\"\"\n    (Tier 3) Overrides T2 behavior to output HTML instead of Markdown.\n    \"\"\"\n    component_name = \"HtmlRefinerPlugin\"\n\n    def _handle_text(self, element: Dict[str, Any], is_header: bool, is_footer: bool) -&gt; List[SayouBlock]:\n        \"\"\"\n        Overrides the _handle_text method.\n        \"\"\"\n        text = element.get(\"text\", \"\").strip()\n        if not text:\n            return []\n\n        raw_attrs = element.get(\"raw_attributes\", {})\n        semantic_type = raw_attrs.get(\"semantic_type\")\n\n        content = \"\"\n        if semantic_type == \"heading\":\n            level = raw_attrs.get(\"heading_level\", 1)\n            content = f\"&lt;h{level}&gt;{text}&lt;/h{level}&gt;\"\n        elif semantic_type == \"list\":\n            content = f\"&lt;li&gt;{text}&lt;/li&gt;\"\n        else:\n            content = f\"&lt;p&gt;{text}&lt;/p&gt;\"\n\n        return [SayouBlock(\n            type=\"html\", \n            content=content, \n            metadata={\n                \"page_num\": element.get(\"meta\", {}).get(\"page_num\"),\n                \"id\": element.get(\"id\"),\n            }\n        )]\n\n# --- Usage ---\n# refiner = HtmlRefinerPlugin()\n# blocks = refiner.refine(doc_data)\n# blocks[0].type == \"html\"\n# blocks[0].content == \"&lt;h1&gt;Heading Text&lt;/h1&gt;\"\n</code></pre>"},{"location":"library-guides/wrapper/","title":"Library Guide: sayou-wrapper","text":"<p><code>sayou-wrapper</code> serves as the standardization layer in the data pipeline. Its primary role is to decouple the \"Data Source\" (Upstream) from the \"Data Consumer\" (Downstream).</p> <p>By enforcing a strict schema (<code>SayouNode</code>), it allows <code>sayou-assembler</code> and <code>sayou-loader</code> to operate purely on graph logic without worrying about field naming conventions or data inconsistencies.</p>"},{"location":"library-guides/wrapper/#1-core-concepts-architecture","title":"1. Core Concepts &amp; Architecture","text":"<p>The library follows the 3-Tier Architecture with a Dynamic Registry Pattern.</p>"},{"location":"library-guides/wrapper/#tier-1-interfaces-the-contract","title":"Tier 1: Interfaces (The Contract)","text":"<ul> <li><code>BaseAdapter</code>: The abstract base class. It defines the <code>adapt()</code> method which takes <code>Any</code> input and returns a <code>WrapperOutput</code> (Pydantic Model).</li> </ul>"},{"location":"library-guides/wrapper/#tier-2-adapters-the-implementations","title":"Tier 2: Adapters (The Implementations)","text":"<ul> <li><code>DocumentChunkAdapter</code>: The default adapter designed to process the output of <code>sayou-chunking</code>.<ul> <li>Maps <code>chunk_id</code> to <code>sayou:doc:{id}</code>.</li> <li>Maps <code>semantic_type=\"table\"</code> to <code>sayou:Table</code> class.</li> <li>Preserves <code>parent_id</code> for graph lineage.</li> </ul> </li> </ul>"},{"location":"library-guides/wrapper/#core-schema-sayounode","title":"Core Schema: <code>SayouNode</code>","text":"<p>This is the lingua franca of the platform.</p> <pre><code>class SayouNode(BaseModel):\n    node_id: str       # Unique URI\n    node_class: str    # Ontology Class\n    attributes: Dict   # Dynamic Properties\n    relationships: Dict[str, List[str]] # Edges\n</code></pre>"},{"location":"library-guides/wrapper/#2-usage-examples","title":"2. Usage Examples","text":""},{"location":"library-guides/wrapper/#21-processing-document-chunks","title":"2.1. Processing Document Chunks","text":"<p>When building a RAG pipeline from documents, <code>sayou-wrapper</code> bridges the gap between text chunks and graph nodes.</p> <pre><code>from sayou.wrapper.pipeline import WrapperPipeline\n\npipeline = WrapperPipeline(adapter_type=\"document_chunk\")\n\n# Input can be a file path or a list of dicts\nnodes = pipeline.run(\"chunks.json\")\n</code></pre>"},{"location":"library-guides/wrapper/#22-extending-for-custom-data-tier-3","title":"2.2. Extending for Custom Data (Tier 3)","text":"<p>You can create custom adapters for structured data like APIs.</p> <pre><code>from sayou.wrapper.interfaces.base_adapter import BaseAdapter\nfrom sayou.wrapper.schemas.sayou_standard import WrapperOutput, SayouNode\n\nclass MyApiAdapter(BaseAdapter):\n    component_name = \"MyApiAdapter\"\n    SUPPORTED_TYPES = [\"my_api\"]\n\n    def _do_adapt(self, raw_data):\n        # Implementation logic...\n        return WrapperOutput(nodes=[...])\n</code></pre>"},{"location":"library-guides/wrapper/#3-data-flow","title":"3. Data Flow","text":"<p>Input (from Chunking):</p> <pre><code>{\n    \"chunk_content\": \"...\",\n    \"metadata\": { \"semantic_type\": \"h1\", \"chunk_id\": \"123\" }\n}\n</code></pre> <p>Output (Standard Node):</p> <pre><code>{\n    \"node_id\": \"sayou:doc:123\",\n    \"node_class\": \"sayou:Topic\",\n    \"attributes\": {\n        \"schema:text\": \"...\",\n        \"sayou:semanticType\": \"h1\"\n    },\n    \"relationships\": {}\n}\n</code></pre>"},{"location":"reference/api-reference/","title":"API Reference","text":"<p>This section provides the auto-generated API documentation for all public classes and methods within the Sayou Data Platform.</p> <p>This documentation is generated directly from the docstrings in the <code>Sayou Data Platform</code> source code using a tool like <code>mkdocstrings</code>.</p>"},{"location":"reference/api-reference/#sayou-core","title":"<code>sayou-core</code>","text":"<p>(Placeholder for <code>mkdocstrings</code> auto-generated content for <code>sayou.core.BaseComponent</code>, <code>sayou.core.Atom</code>, etc.)</p>"},{"location":"reference/api-reference/#sayou-connector","title":"<code>sayou-connector</code>","text":"<p>(Placeholder for <code>mkdocstrings</code> auto-generated content for <code>sayou.connector.interfaces.BaseFetcher</code>, <code>sayou.connector.fetcher.ApiFetcher</code>, etc.)</p>"},{"location":"reference/api-reference/#sayou-chunking","title":"<code>sayou-chunking</code>","text":"<p>(Placeholder for <code>mkdocstrings</code> auto-generated content for <code>sayou.chunking.interfaces.BaseSplitter</code>, <code>sayou.chunking.splitter.RecursiveCharacterSplitter</code>, etc.)</p>"},{"location":"reference/api-reference/#sayou-llm","title":"<code>sayou-llm</code>","text":"<p>(Placeholder for <code>mkdocstrings</code> auto-generated content for <code>sayou.llm.interfaces.BaseLLMClient</code>, <code>sayou.llm.clients.OpenAIClient</code>, <code>sayou.llm.clients.LlamaCppClient</code>, etc.)</p>"},{"location":"reference/api-reference/#sayou-rag","title":"<code>sayou-rag</code>","text":"<p>(Placeholder for <code>mkdocstrings</code> auto-generated content for <code>sayou.rag.executor.RAGExecutor</code>, <code>sayou.rag.interfaces.BaseRouter</code>, <code>sayou.rag.interfaces.BaseFetcher</code>, etc.)</p> <p>(...and so on for all 10 libraries...)</p>"},{"location":"reference/contributing/","title":"Contributing","text":"<p>We welcome contributions to the Sayou Data Platform! Whether you're fixing a bug, proposing a new feature, or improving documentation, your help is valued.</p>"},{"location":"reference/contributing/#ways-to-contribute","title":"Ways to Contribute","text":"<ul> <li>Issues: Report bugs, request new features, or ask questions. Please check if a similar issue already exists before creating a new one.</li> <li>Pull Requests (PRs): Submit your changes to the codebase.</li> </ul>"},{"location":"reference/contributing/#pull-request-workflow","title":"Pull Request Workflow","text":"<p>We follow the Git Strategy outlined in the main <code>README.md</code>. All contributions must be made via Pull Requests to the <code>develop</code> branch.</p> <ol> <li>Fork the <code>sayou-fabric</code> repository to your own GitHub account.</li> <li>Clone your fork to your local machine.</li> <li>Create a branch from <code>develop</code>. Use the <code>feature/</code> or <code>fix/</code> prefix (e.g., <code>git checkout -b feature/add-new-fetcher</code>).</li> <li>Make your changes.</li> <li>Add Tests: (Placeholder for text: \"If you are adding a new feature or fixing a bug, please add corresponding unit tests under the <code>tests/</code> directory for that package.\")</li> <li>Update Documentation: (Placeholder for text: \"If your change affects behavior, please update the relevant <code>.md</code> file in the <code>docs/library-guides/</code> folder.\")</li> <li>Run Linters/Formatters: (Placeholder for text: \"Please run <code>black .</code> and <code>ruff .</code> to format your code before committing.\")</li> <li>Commit your changes with a conventional commit message (e.g., <code>feat(connector): Add S3Fetcher T3 plugin</code>).</li> <li>Push your branch to your fork.</li> <li>Open a Pull Request from your branch to the <code>sayou-fabric:develop</code> branch.</li> </ol>"},{"location":"reference/contributing/#development-setup","title":"Development Setup","text":"<p>(Placeholder for text: \"This section will detail how to set up the Monorepo for local development. This includes installing all packages in editable mode and managing shared dependencies.\")</p>"},{"location":"reference/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>(Placeholder for text: \"All contributors are expected to follow our Code of Conduct. Please be respectful and constructive.\")</p>"},{"location":"reference/contributing/#license","title":"License","text":"<p>By contributing to <code>sayou-fabric</code>, you agree that your contributions will be licensed under the Apache 2.0 License that covers the project.</p>"},{"location":"sayou-agent/overview/","title":"Sayou RAG: The Data Platform Orchestrator","text":"<p>Welcome to Sayou RAG. This is the command center of the Sayou Data Platform.</p> <p>If individual libraries like <code>sayou-document</code> or <code>sayou-chunking</code> are the engine parts (pistons, gears, valves), then <code>sayou-rag</code> is the Car. It assembles these parts into a working vehicle that takes you from \"Raw Data\" to \"Intelligent Answers.\"</p>"},{"location":"sayou-agent/overview/#1-the-two-pillars-of-rag","title":"1. The Two Pillars of RAG","text":"<p>We architected <code>sayou-rag</code> around two fundamental lifecycles of data.</p>"},{"location":"sayou-agent/overview/#phase-1-ingestion-pipeline-the-builder","title":"Phase 1: Ingestion Pipeline (The Builder)","text":"<ul> <li>Goal: Turn unstructured chaos into structured knowledge.</li> <li>Process: This is a heavy, batch-oriented process.<ol> <li>Connect: Fetch data from files or APIs.</li> <li>Parse: Extract high-fidelity data (<code>sayou-document</code>).</li> <li>Refine: Convert to standard Markdown (<code>sayou-refinery</code>).</li> <li>Chunk: Split based on structure and context (<code>sayou-chunking</code>).</li> <li>Wrap: Enforce company schema (<code>sayou-wrapper</code>).</li> <li>Assemble: Build the Knowledge Graph (<code>sayou-assembler</code>).</li> <li>Load: Persist to Vector DB or Graph DB (<code>sayou-loader</code>).</li> </ol> </li> </ul>"},{"location":"sayou-agent/overview/#phase-2-inference-pipeline-the-solver","title":"Phase 2: Inference Pipeline (The Solver)","text":"<ul> <li>Goal: Retrieve precise context and generate answers.</li> <li>Process: This is a real-time, latency-sensitive process.<ol> <li>Extract: Search KG/Vector Store for relevant nodes (<code>sayou-extractor</code>).</li> <li>Generate: Synthesize answers using LLMs (<code>sayou-llm</code>).</li> </ol> </li> </ul>"},{"location":"sayou-agent/overview/#2-smart-routing-automation","title":"2. Smart Routing &amp; Automation","text":"<p>The core innovation of <code>sayou-rag</code> is the <code>StandardPipeline</code>. It acts as an intelligent router that decides how to process input data without user intervention.</p> <p>Scenario A: \"I have a PDF file.\"</p> <pre><code>rag.ingest(\"manual.pdf\")\n</code></pre> <ul> <li>Logic: Detects file path -&gt; Activates <code>DocumentPipeline</code> -&gt; Full ETL Process.</li> </ul> <p>Scenario B: \"I have raw text or JSON.\"</p> <pre><code>rag.ingest({\"text\": \"...\"})\n</code></pre> <ul> <li>Logic: Detects Dict input -&gt; Skips Parsing -&gt; Activates <code>WrapperPipeline</code> directly.</li> </ul>"},{"location":"sayou-agent/overview/#3-dependency-map","title":"3. Dependency Map","text":"<p><code>sayou-rag</code> sits at the top of the hierarchy. It does not contain complex parsing or splitting logic itself; it imports and orchestrates them. * Upstream: <code>sayou-connector</code> * Midstream (Processing): <code>sayou-document</code>, <code>sayou-refinery</code>, <code>sayou-chunking</code> * Midstream (Structure): <code>sayou-wrapper</code>, <code>sayou-assembler</code> * Downstream: <code>sayou-loader</code>, <code>sayou-extractor</code>, <code>sayou-llm</code></p>"},{"location":"sayou-agent/overview/#4-getting-started","title":"4. Getting Started","text":"<p>For detailed usage of the pipeline, refer to the Quickstart guide in the README or explore the specific guides for each component library in the left sidebar.</p> <p>To begin building your own RAG application, simply install the main package:</p> <pre><code>pip install sayou-rag\n</code></pre> <p>And initialize the standard pipeline:</p> <pre><code>from sayou.rag.pipeline.standard import StandardPipeline\nrag = StandardPipeline()\nrag.initialize()\n</code></pre>"}]}